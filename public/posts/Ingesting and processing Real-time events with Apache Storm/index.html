<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Ingesting and processing Real-time events with Apache Storm &middot; saptak.github.io </title>

  
  <link rel="stylesheet" href="http://localhost:1313/css/poole.css">
  <link rel="stylesheet" href="http://localhost:1313/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:1313/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="" rel="alternate" type="application/rss+xml" title="saptak.github.io" />
</head>

<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>saptak.github.io</h1>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
    </ul>

    <p>&copy; 2015. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="post">
  <h1>Ingesting and processing Real-time events with Apache Storm</h1>
  <span class="post-date">Fri, Feb 6, 2015</span>
      

<p>Trucking business is a high-risk business where truck drivers venture into remote areas, often despite harsh weather conditions and chaotic traffic on a daily basis. Using this solution illustrating Modern Data Archtecture with Hortonworks Data Platform, we have developed a centralized management system that can help reduce risk and lower the total cost of operations. This system can take into consideration adverse weather conditions, the driver’s driving patterns, current traffic conditions and other criteria to alert and inform the management staff and the drivers themselves when risk factors run high.</p>

<p>In a <a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">previous tutorial</a> we learned to collect this data using Apache Kafka.</p>

<p>In this tutorial we will use <a href="http://hortonworks.com/labs/storm/"><strong>Apache Storm</strong></a> on the Hortonworks Data Platform to capture these data events and process them in real time for further analysis.</p>

<h3 id="technologies-used:a1b3bc07fb00147f3ac2e3513fe5d148">Technologies Used</h3>

<p>Hadoop, HDFS, Hive, HBase, Kafka, Storm, Falcon, Leafletjs.</p>

<h3 id="data-sets-used:a1b3bc07fb00147f3ac2e3513fe5d148">Data Sets Used</h3>

<ul>
<li>New York City Truck Routes from NYC DOT.</li>
<li>Truck Events Data generated using a custom simulator.</li>
<li>Weather Data, collected using APIs from Forcast.io.</li>
<li>Traffic Data, collected using APIs from MapQuest.</li>
</ul>

<p><em>All data sets used in these tutorials are real data sets but modified to fit these use cases.</em></p>

<h3 id="about-storm:a1b3bc07fb00147f3ac2e3513fe5d148">About Storm</h3>

<p>Apache Storm is an Open Source distributed, reliable, fault tolerant system for real time processing of data at high velocity.</p>

<p>It’s used for:</p>

<ul>
<li>Real time analytics</li>
<li>Online machine learning</li>
<li>Continuous statics computations</li>
<li>Operational Analytics</li>
<li>And, to enforce Extract, Transform, and Load (ETL) paradigms.</li>
</ul>

<p>Spout and Bolt are the two main components in Storm, which work together to process streams of data.</p>

<ul>
<li>Spout: Works on the source of data streams. In the &ldquo;Truck Events&rdquo; use case, Spout will read data from Kafka “truckevent” topics.</li>
<li>Bolt: Spout passes streams of data to Bolt which processes and passes it to either a data store or another Bolt.</li>
</ul>

<p>For details on Storm, <a href="http://hortonworks.com/labs/storm/">click here</a>.</p>

<p>In this tutorial, you will learn the following topics:</p>

<ol>
<li>Managing Storm on HDP.</li>
<li>Creating a Storm spout to consume the Kafka ‘truckevents’ generated in <a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Tutorial #1</a>.</li>
</ol>

<h2 id="prerequisites:a1b3bc07fb00147f3ac2e3513fe5d148">Prerequisites</h2>

<p><a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Tutorial #1 should be completed successfully.</a></p>

<h3 id="step-1-configure-storm:a1b3bc07fb00147f3ac2e3513fe5d148">Step 1: <strong>Configure Storm.</strong></h3>

<p>Verify if Apache Storm is installed and started by login into Ambari</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image01.png" alt="" />
</p>

<p>Note: If you do not see Storm listed under Services, please follow click on Action-&gt;Add Service and select Storm and deploy it.</p>

<h4 id="check-storm-configurations-on-the-sandbox-by-login-into-ambari:a1b3bc07fb00147f3ac2e3513fe5d148">Check Storm configurations on the Sandbox by login into Ambari.</h4>

<ul>
<li>Zookeeper configuration:<br /></li>
</ul>

<p>Ensure storm.zookeeper.servers is set to <code>sandbox.hortonworks.com</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image04.png" alt="" />
</p>

<ul>
<li>Check the local directory configuration:<br /></li>
</ul>

<p>Ensure storm.local.dir is set to <code>/hadoop/storm</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image07.png" alt="" />
</p>

<ul>
<li>Check the nimbus host configuration:<br /></li>
</ul>

<p>Ensure nimbus.host is set to <code>sandbox.hortonworks.com</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image10.png" alt="" />
</p>

<ul>
<li>Check the slots allocated:<br /></li>
</ul>

<p>Ensure supervisor.slots.ports is set to <code>[6700, 6701]</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image13.png" alt="" />
</p>

<ul>
<li>Check the UI configuration port:<br /></li>
</ul>

<p>Ensure ui.port is set to <code>8744</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image16.png" alt="" />
</p>

<h4 id="check-the-storm-ui-from-the-quick-links:a1b3bc07fb00147f3ac2e3513fe5d148">Check the Storm UI from the Quick Links</h4>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image19.png" alt="" />
</p>

<p>Now you can see the UI:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/Storm+UI.png" alt="Storm UI" />
<br />
Storm UI</p>

<h3 id="step-2-creating-a-storm-spout-to-consume-the-kafka-truck-events-generated-in-tutorial-1:a1b3bc07fb00147f3ac2e3513fe5d148">Step 2. <strong>Creating a Storm Spout to consume the Kafka truck events generated in Tutorial #1.</strong></h3>

<h4 id="new-york-city-truck-routes:a1b3bc07fb00147f3ac2e3513fe5d148">New York City truck routes:</h4>

<p>The required <a href="http://www.nyc.gov/html/dot/downloads/misc/all_truck_routes_nyc.kml">New York City truck routes</a> KML file is included in the master.zip file. If required you can download the latest copy of the file with the following command. </p>

<pre><code>[root@sandbox ~]# wget http://www.nyc.gov/html/dot/downloads/misc/all_truck_routes_nyc.kml --directory-prefix=/opt/TruckEvents/Tutorials-master/src/main/resources/
</code></pre>

<p>Compile the code using Maven after downloading a new data file or on completing any changes to the code under <code>/opt/TruckEvents/Tutorials-master/src</code> directory.</p>

<pre><code>[root@sandbox ~]# cd /opt/TruckEvents/Tutorials-master/  
[root@sandbox ~]# export PATH=/usr/local/apache-maven-3.2.2/bin:$PATH  
[root@sandbox ~]# mvn clean package
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/mvn+clean+package.png" alt="mvn clean package" />
<br />
mvn clean package<img src="http://hortonassets.s3.amazonaws.com/storm-truck/Build+Success.png" alt="mvn build success" />
<br />
mvn build success</p>

<p>We now have a successfully compiled the code.</p>

<p>Look through the Java code under ‘src/main/java’ directory.</p>

<h4 id="verify-that-kafka-process-is-running:a1b3bc07fb00147f3ac2e3513fe5d148">Verify that Kafka process is running</h4>

<p>After successful completion of Tutorial #1, we should now have a running instance of Kafka. To verify execute ‘jps’ command to find Kafka in the running Java processes.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/Kafka+Running.png" alt="Kafka Running" />
<br />
Kafka Running</p>

<p>If Kafka is not running, you can start by login into Ambari (Follow instruction in the previous tutorial)</p>

<h4 id="loading-storm-topology:a1b3bc07fb00147f3ac2e3513fe5d148">Loading Storm Topology</h4>

<p>We now have ‘supervisor’ daemon and Kafka processes running.</p>

<p>The command below will start a new Storm Topology for TruckEvents.</p>

<pre><code>[root@sandbox ~]# [root@sandbox ~]# storm jar target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial2.TruckEventProcessingTopology
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/storm+new+topology.png" alt="storm new topology" />
<br />
storm new topology</p>

<p>Refresh the browser to see new Topology ‘truck-event-processor’ in the browser.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/truck+event+topology.png" alt="truck event processor new topology" />
<br />
truck event processor new topology</p>

<h4 id="generationg-truckevents:a1b3bc07fb00147f3ac2e3513fe5d148">Generationg TruckEvents</h4>

<p>The TruckEvents producer can now be executed as we did in Tutorial #1.</p>

<pre><code>java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer  localhost:9092 localhost:2181
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/TruckEvents+Producer.png" alt="Truck Events Producer" />
<br />
Truck Events Producer</p>

<p>Refresh the browser and you can see that messages are processed in real time by Spout.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/Storm+spout+messages+count.png" alt="kafkaSpout count" />
<br />
kafkaSpout count</p>

<h3 id="code-description:a1b3bc07fb00147f3ac2e3513fe5d148">Code description</h3>

<p>Let us review the code used in this tutorial. The source files are under the <code>/opt/TruckEvents/Tutorials-master/src/main/java/tutorial2</code> folder. </p>

<pre><code>[root@sandbox Tutorials-master]# ls -l src/main/java/tutorial2/  
total 16  
-rw-r--r-- 1 root root  861 Jul 24 23:34 BaseTruckEventTopology.java  
-rw-r--r-- 1 root root 1205 Jul 24 23:34 LogTruckEventsBolt.java  
-rw-r--r-- 1 root root 2777 Jul 24 23:34 TruckEventProcessingTopology.java  
-rw-r--r-- 1 root root 2233 Jul 24 23:34 TruckScheme.java  
[root@sandbox Tutorials-master]# 
</code></pre>

<h4 id="basetruckeventtopology-java:a1b3bc07fb00147f3ac2e3513fe5d148">BaseTruckEventTopology.java</h4>

<pre><code>topologyConfig.load(ClassLoader.getSystemResourceAsStream(configFileLocation));
</code></pre>

<p>Is the base class, where the topology configurations is initialized from the /resource/truck_event_topology.properties files.</p>

<h4 id="truckeventprocessingtopology-java:a1b3bc07fb00147f3ac2e3513fe5d148">TruckEventProcessingTopology.java</h4>

<p>This is the storm topology configuration class, where the Kafka spout and LogTruckevent Bolts are initialized. In the following method the Kafka spout is configured.</p>

<pre><code>private SpoutConfig constructKafkaSpoutConf()  
    {
 …  
 SpoutConfig spoutConfig = new SpoutConfig(hosts, topic, zkRoot, consumerGroupId);  
…
        spoutConfig.scheme = new SchemeAsMultiScheme(new TruckScheme());

return spoutConfig;  
    }
</code></pre>

<p>A logging bolt that prints the message from the Kafka spout was created for debugging purpose just for this tutorial.</p>

<p><code>
public void configureLogTruckEventBolt(TopologyBuilder builder)  
{  
LogTruckEventsBolt logBolt = new LogTruckEventsBolt();  
builder.setBolt(LOG_TRUCK_BOLT_ID, logBolt).globalGrouping(KAFKA_SPOUT_ID);  
}  
</code></p>

<p>The topology is built and submitted in the following method;</p>

<p><code>
private void buildAndSubmit() throws Exception  
{  
...  
StormSubmitter.submitTopology(&quot;truck-event-processor&quot;,  
conf, builder.createTopology());  
}  
</code></p>

<h4 id="truckscheme-java:a1b3bc07fb00147f3ac2e3513fe5d148">TruckScheme.java</h4>

<p>Is the deserializer provided to the kafka spout to deserialize kafka byte message stream to Values objects.</p>

<pre><code>public List&lt;Object&gt; deserialize(byte[] bytes)  
        {
        try  
                {
            String truckEvent = new String(bytes, &quot;UTF-8&quot;);  
            String[] pieces = truckEvent.split(&quot;\\|&quot;);

            Timestamp eventTime = Timestamp.valueOf(pieces[0]);  
            String truckId = pieces[1];  
            String driverId = pieces[2];  
            String eventType = pieces[3];  
            String longitude= pieces[4];  
            String latitude  = pieces[5];  
            return new Values(cleanup(driverId), cleanup(truckId),  
                                    eventTime, cleanup(eventType), cleanup(longitude), cleanup(latitude));

        }  
                catch (UnsupportedEncodingException e)  
                {
                    LOG.error(e);  
                    throw new RuntimeException(e);  
        }

    }
</code></pre>

<h4 id="logtruckeventsbolt-java:a1b3bc07fb00147f3ac2e3513fe5d148">LogTruckEventsBolt.java</h4>

<p>LogTruckEvent spout logs the kafka message received from the kafka spout to the log files under /var/log/storm/worker-*.log</p>

<p><code>
public void execute(Tuple tuple)  
{  
LOG.info(tuple.getStringByField(TruckScheme.FIELD_DRIVER_ID) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_TRUCK_ID) + &quot;,&quot; +  
tuple.getValueByField(TruckScheme.FIELD_EVENT_TIME) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_EVENT_TYPE) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_LATITUDE) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_LONGITUDE));  
}  
</code></p>

<p>In this tutorial we have learned to capture data from Kafka Producer into Storm Spout. This data can now be processed in real time. In our next Tutorial, using Storm Bolt we will learn to store data into multiple sources to persist.</p>

</div>
</div>

  </body>
</html>
