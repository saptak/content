<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> saptak.github.io &middot; saptak.github.io </title>

  
  <link rel="stylesheet" href="http://localhost:1313/css/poole.css">
  <link rel="stylesheet" href="http://localhost:1313/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:1313/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://localhost:1313/index.xml" rel="alternate" type="application/rss+xml" title="saptak.github.io" />
</head>

<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>saptak.github.io</h1>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
    </ul>

    <p>&copy; 2015. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/Real%20time%20Data%20Ingestion%20in%20HBase%20-%20Hive%20using%20Storm%20Bolt/">
        Real time Data Ingestion in HBase &amp; Hive using Storm Bolt
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<h2 id="overview:055c69fb56cd76d281fd68883ff1c99f">Overview</h2>

<p>In this tutorial, we will build a solution to ingest real time streaming data into HBase and HDFS.</p>

<p>In previous tutorial we have explored generating and processing streaming data with <a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Apache Kafka</a>and <a href="http://hortonworks.com/hadoop-tutorial/ingesting-processing-real-time-events-apache-storm/">Apache Storm</a>. In this tutorial we will create HDFS Bolt &amp; HBase Bolt to read the streaming data from the Kafka Spout and persist in Hive &amp; HBase tables. </p>

<h3 id="about-hbase:055c69fb56cd76d281fd68883ff1c99f">About HBase</h3>

<p>HBase provides near real-time, random read and write access to tables (or to be more accurate ‘maps’) storing billions of rows and millions of columns.</p>

<p>In this case once we store this rapidly and continuously growing dataset from Internet of Things (IoT), we will be able to do super fast lookup for analytics irrespective of the data size.</p>

<h3 id="about-storm:055c69fb56cd76d281fd68883ff1c99f">About Storm</h3>

<p>Apache Storm is an Open Source distributed, reliable, fault – tolerant system for real time processing of large volume of data. Spout and Bolt are the two main components in Storm, which work together to process streams of data.</p>

<ul>
<li>Spout: Works on the source of data streams. In the &ldquo;Truck Events&rdquo; use case, Spout will read data from Kafka topics.</li>
<li>Bolt: Spout passes streams of data to Bolt which processes and passes it to either a data store or another Bolt.</li>
</ul>

<p>In this tutorial, you will learn the following topics:</p>

<ul>
<li>To configure Storm Bolt.</li>
<li>Store Persisting data in HBase and Hive.</li>
<li>Verify data in HDFS and HBase.</li>
</ul>

<h2 id="prerequisites:055c69fb56cd76d281fd68883ff1c99f">Prerequisites</h2>

<p><a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Tutorial #1</a> &amp; <a href="http://hortonworks.com/hadoop-tutorial/ingesting-processing-real-time-events-apache-storm/">Tutorial #2</a> should be completed successfully with a functional Storm and Kafka Bolt reading data from the Kafka Queue.</p>

<h3 id="step-1:055c69fb56cd76d281fd68883ff1c99f">Step 1:</h3>

<p><strong>Smoke Test running Services.</strong></p>

<ul>
<li><strong>Check NameNode and DataNode Service.</strong></li>
</ul>

<p>The _‘jps’_ command will show all java process that are currently running as seen in the screenshot below.</p>

<p>Verify that the _NameNode_ and the _DataNode_ processes are running.</p>

<p><code>
[root@sandbox ~]# jps  
</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/jps.png" alt="jps" />
</p>

<ul>
<li><strong>Check that HBase is running.</strong></li>
</ul>

<p>If <strong>HMaster</strong> and <strong>HRegionServer</strong> are missing in the ‘jps’ command output list, as seen above, we need to restart the HBase services from Ambari.</p>

<p>‘jps’ command output list should now show that the <strong>HMaster</strong> and <strong>HRegionServer</strong> services are running.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/jps+hbase+running.png" alt="jps : verify hbase services running" />
</p>

<p>jps : verify hbase services running</p>

<p>To smoke test HBase, we need to login to _HBase_ user account and start the HBase shell to verify the status of the services.</p>

<pre><code>[root@sandbox ~]# su hbase  
[hbase@sandbox root]$ hbase shell

hbase(main):001:0&gt; status

hbase(main):002:0&gt; exit

[hbase@sandbox root]$ exit  
[root@sandbox ~]# 
</code></pre>

<p>The output is as shown in the screenshot below.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/hbase+running.png" alt="hbase running" />
</p>

<p>hbase running</p>

<ul>
<li><strong>Smoke test that Hive is Running.</strong></li>
</ul>

<p>You can smoke test Hive by either using the command line or by browser.</p>

<p>To smoke test using the command line, login as a Hive user with _su hive_ command and follow the instructions shown below:</p>

<pre><code>[root@sandbox ~]# su hive  
[hive@sandbox root]$ hive

hive&gt; show databases;

hive&gt; exit;  
[hive@sandbox root]$ exit

[root@sandbox ~]# 
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/smoketest+hive.png" alt="smoketest hive" />
</p>

<p>smoketest hive</p>

<p>To smoke test Hive by using a browser, open the url ‘<a href="http://localhost:8000/beeswax/’">http://localhost:8000/beeswax/’</a> from any browser on your local machine.</p>

<p>Execute the query &ldquo;<strong>show databases;</strong>&rdquo; in the query editor window.</p>

<p>Click on ‘Execute’ button to see an output as shown in the screenshot below.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/show+databases.png" alt="Query Editor" />
</p>

<p>Hive queries can be tested and saved using the Query Editor.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/default+databases.png" alt="default database" />
</p>

<p>default database</p>

<h3 id="step-2:055c69fb56cd76d281fd68883ff1c99f">Step 2:</h3>

<p><strong>Persist data in HDFS &amp; HBase.</strong></p>

<ul>
<li><strong>Creating HBase tables</strong></li>
</ul>

<p>We work with have 2 Hbase tables in this tutorial.</p>

<p>The first table stores all events generated and the second stores the ‘driverId’ and non-normal events count.</p>

<p>As with Hive, we can execute HBase queries via a browser.</p>

<p>Use the following url to open a HBase shell: <a href="http://localhost:8000/shell/create?keyName=hbase. ">http://localhost:8000/shell/create?keyName=hbase. </a></p>

<pre><code>hbase(main):001:0&gt; create 'truck_events', 'events'  
hbase(main):002:0&gt; create 'driver_dangerous_events', 'count'  
hbase(main):003:0&gt; list  
hbase(main):004:0&gt; 
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/hbase+create+tables.png" alt="hbase create tables" />
</p>

<p>hbase create tables</p>

<p>Next, we will create Hive tables.</p>

<ul>
<li><strong>Creating Hive tables</strong></li>
</ul>

<p>Open the URL <a href="http://localhost:8000/beeswax/">http://localhost:8000/beeswax/</a> in a browser and copy the below script into the query editor:</p>

<pre><code>create table truck_events_text_partition  
(driverId string,  
 truckId string,  
 eventTime timestamp,  
 eventType string,  
 longitude double,  
 latitude double)  
partitioned by (date string)  
ROW FORMAT DELIMITED  
FIELDS TERMINATED BY ',';
</code></pre>

<p>This script creates the Hive table to persist all events generated. This table is partitioned by date.</p>

<p>The table created can be viewed at this URL: <a href="http://localhost:8000/beeswax/table/default/truck_events_text_partition">http://localhost:8000/beeswax/table/default/truck_events_text_partition</a></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/Screen+Shot+2014-08-15+at+12.00.48+AM.png" alt="Hive table" />
</p>

<p>Hive table</p>

<p>Verify that the table has been properly created by clicking Tables and selecting truck_events_text_partiiton.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image22.png" alt="" />
<br />
<img src="http://hortonassets.s3.amazonaws.com/mda/tut3/truck_events_text_partition+table+created.png" alt="truck_events_text_partition" />
<br />
truck_events_text_partition</p>

<ul>
<li><strong>Creating ORC ‘truckevent’ Hive tables</strong></li>
</ul>

<p>The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. It was designed to overcome limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.</p>

<p>Syntax for ORC tables:</p>

<p>CREATE TABLE … STORED AS ORC</p>

<p>ALTER TABLE … [PARTITION partition_spec] SET FILEFORMAT ORC</p>

<p><em>Note: This statement only works on partitioned tables. If you apply it to flat tables, it may cause query errors.</em></p>

<p>SET hive.default.fileformat=Orc</p>

<p>Let us create the ‘truckevent’ table as per the above syntax:</p>

<p><code>
create table truck_events_text_partition_orc  
(driverId string,  
truckId string,  
eventTime timestamp,  
eventType string,  
longitude double,  
latitude double)  
partitioned by (date string)  
ROW FORMAT DELIMITED  
FIELDS TERMINATED BY ','  
stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);  
</code></p>

<p>The data in ‘truck_events_text_partition_orc’ table can be stored with ZLIB, Snappy, LZO compression options. This can be set by changing <code>tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;)</code>option in the query above. </p>

<h3 id="step-3:055c69fb56cd76d281fd68883ff1c99f">Step 3:</h3>

<p><strong>Updating Tutorials-master Project</strong></p>

<ul>
<li>Copy /etc/hbase/conf/hbase-site.xml to src/main/resources/ directory and recompile the Maven project.</li>
</ul>

<p>[root@sandbox ~]# cd /opt/TruckEvents/Tutorials-master/</p>

<p>[root@sandbox Tutorials-master]# cp /etc/hbase/conf/hbase-site.xml src/main/resources/</p>

<p>[root@sandbox Tutorials-master]# mvn clean package</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/mvn+clean+package.png" alt="update project" />
</p>

<p>In case, mvn is not in your path, you can use the command <code>export PATH=/usr/local/apache-maven-3.2.2/bin:$PATH</code> to include it in your path.</p>

<ul>
<li>Deactivate &amp; Kill the Storm topology using the Storm UI as shown in the screenshot below:<br /></li>
</ul>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/storm+UI.png" alt="storm UI" />
</p>

<p>storm UI<img src="http://hortonassets.s3.amazonaws.com/mda/tut3/Deactivate+and+Kill.png" alt="deactivate and kill" />
</p>

<p>deactivate and kill<img src="http://hortonassets.s3.amazonaws.com/mda/tut3/deactivate.png" alt="Deactivate" />
</p>

<p>Deactivate<img src="http://hortonassets.s3.amazonaws.com/mda/tut3/kill.png" alt="kill" />
</p>

<p>kill</p>

<ul>
<li><strong>Loading new Storm topology.</strong></li>
</ul>

<p>Execute the Storm ‘jar’ command to create a new Topology from <strong>Tutorial# 3</strong> after the code has been compiled.</p>

<pre><code>[root@sandbox Tutorials-master]# storm jar target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial3.TruckEventProcessingTopology
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/Topology+Summary.png" alt="Topology Summary" />
</p>

<p>Topology Summary</p>

<h3 id="step-4:055c69fb56cd76d281fd68883ff1c99f">Step 4:</h3>

<p><strong>Verify Data in HDFS and HBase.</strong></p>

<ul>
<li>Start the <strong>‘TruckEventsProducer’</strong> Kafka Producer and verify that the data has been persisted by using the Storm Topology view.</li>
</ul>

<p>[root@sandbox Tutorials-master]# java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer localhost:9092 localhost:2181</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/Verify+data+in+Storm+UI.png" alt="Verify Storm UI" />
</p>

<p>Verify Storm UI</p>

<ul>
<li>Verify that the data is in HBase by executing the following commands in HBase shell:</li>
</ul>

<p>hbase(main):001:0&gt; list</p>

<p>hbase(main):002:0&gt; count ‘truck_events’</p>

<p>366 row(s) in 0.3900 seconds</p>

<p>=&gt; 366</p>

<p>hbase(main):003:0&gt; count ‘driver_dangerous_events’</p>

<p>3 row(s) in 0.0130 seconds</p>

<p>=&gt; 3</p>

<p>hbase(main):004:0&gt; exit</p>

<p>The ‘driver_dangerous_events’ table is updated upon every violation.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/Verify+data+in+HBase.png" alt="Verify data in HBase" />
</p>

<p>Verify data in HBase</p>

<ul>
<li>Verify that the data is in HDFS by browsing to this URL: <a href="http://localhost:8000/filebrowser/view/truck-events-v4/staging">http://localhost:8000/filebrowser/view/truck-events-v4/staging</a><br /></li>
</ul>

<p>We should see the files been injested in HDFS now.</p>

<p>With the default settings for HDFS, users might see the data written to HDFS once in every 5 minutes.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/tut3/Verify+Data+in+HDFS.png" alt="Verify data in HDFS" />
</p>

<p>Verify data in HDFS</p>

<p>This completes the tutorial #3. </p>

<p>In the next tutorial we will integrate with a UI and visualize the events in real time. </p>

<h3 id="code-description:055c69fb56cd76d281fd68883ff1c99f">Code Description</h3>

<p>1.<strong>BaseTruckEventTopology.java</strong></p>

<pre><code>topologyConfig.load(ClassLoader.getSystemResourceAsStream(configFileLocation));
</code></pre>

<p>This is the base class, where the topology configuration is initialized from the /resource/truck_event_topology.properties files.</p>

<p>2.<strong>FileTimeRotationPolicy.java</strong></p>

<p>This implements the file rotation policy after a certain duration.</p>

<p>“`</p>

<p>public FileTimeRotationPolicy(float count, Units units) {</p>

<p>this.maxMilliSeconds = (long) (count * units.getMilliSeconds());</p>

<p>}</p>

<pre><code>@Override  
public boolean mark(Tuple tuple, long offset) {  
    // The offsett is not used here as we are rotating based on time  
    long diff = (new Date()).getTime() - this.lastCheckpoint;  
    return diff &gt;= this.maxMilliSeconds;  
}
</code></pre>

<p>3.<strong>LogTruckEventsBolt.java</strong></p>

<p>LogTruckEvent Spout logs the Kafka messages received from the Kafka Spout to the log files under /var/log/storm/worker-*.log</p>

<pre><code>public void execute(Tuple tuple)  
 {
 LOG.info(tuple.getStringByField(TruckScheme.FIELD_DRIVER_ID) + &quot;,&quot; +  
 tuple.getStringByField(TruckScheme.FIELD_TRUCK_ID) + &quot;,&quot; +  
 tuple.getValueByField(TruckScheme.FIELD_EVENT_TIME) + &quot;,&quot; +  
 tuple.getStringByField(TruckScheme.FIELD_EVENT_TYPE) + &quot;,&quot; +  
 tuple.getStringByField(TruckScheme.FIELD_LATITUDE) + &quot;,&quot; +  
 tuple.getStringByField(TruckScheme.FIELD_LONGITUDE));  
 }
</code></pre>

<p>4.<strong>TruckScheme.java</strong></p>

<p>This is the deserializer provided to the Kafka Spout to deserialize Kafka’s byte message streams to Values objects.</p>

<pre><code>public List&lt;Object&gt; deserialize(byte[] bytes)  
        {
        try  
                {
            String truckEvent = new String(bytes, &quot;UTF-8&quot;);  
            String[] pieces = truckEvent.split(&quot;\\|&quot;);

            Timestamp eventTime = Timestamp.valueOf(pieces[0]);  
            String truckId = pieces[1];  
            String driverId = pieces[2];  
            String eventType = pieces[3];  
            String longitude= pieces[4];  
            String latitude  = pieces[5];  
            return new Values(cleanup(driverId), cleanup(truckId),  
                                    eventTime, cleanup(eventType), cleanup(longitude), cleanup(latitude));

        }  
                catch (UnsupportedEncodingException e)  
                {
                    LOG.error(e);  
                    throw new RuntimeException(e);  
        }

    }
</code></pre>

<p>5.<strong>HiveTablePartitionAction.java</strong></p>

<p>This creates Hive partitions based on timestamp and loads the data by executing the Hive DDL statements.</p>

<pre><code>public void loadData(String path, String datePartitionName, String hourPartitionName )  
    {

        String partitionValue = datePartitionName + &quot;-&quot; + hourPartitionName;

        LOG.info(&quot;About to add file[&quot;+ path + &quot;] to a partitions[&quot;+partitionValue + &quot;]&quot;);

        StringBuilder ddl = new StringBuilder();  
        ddl.append(&quot; load data inpath &quot;)  
            .append(&quot; '&quot;).append(path).append(&quot;' &quot;)  
            .append(&quot; into table &quot;)  
            .append(tableName)  
            .append(&quot; partition &quot;).append(&quot; (date='&quot;).append(partitionValue).append(&quot;')&quot;);

        startSessionState(sourceMetastoreUrl);
</code></pre>

<p>The data is stored in the partitioned ORC tables using the following method.</p>

<pre><code>String ddlORC = &quot;INSERT OVERWRITE TABLE &quot; + tableName + &quot;_orc SELECT * FROM &quot; +tableName;




    try {  
        execHiveDDL(&quot;use &quot; + databaseName);  
        execHiveDDL(ddl.toString());  
        execHiveDDL(ddlORC.toString());  
    } catch (Exception e) {  
        String errorMessage = &quot;Error exexcuting query[&quot;+ddl.toString() + &quot;]&quot;;  
        LOG.error(errorMessage, e);  
        throw new RuntimeException(errorMessage, e);  
    }
} 
</code></pre>

<p>6.<strong>TruckEventProcessingTopology.java</strong></p>

<p>This creates a connection to HBase tables and access data within the prepare() function.</p>

<pre><code>public void prepare(Map stormConf, TopologyContext context,  
 OutputCollector collector)  
 {
 ...  
 this.connection = HConnectionManager.createConnection(constructConfiguration());  
 this.eventsCountTable = connection.getTable(EVENTS_COUNT_TABLE_NAME);  

 this.eventsTable = connection.getTable(EVENTS_TABLE_NAME);  
 } 



...  
}



Data to be stored is prepared in the constructRow() function using put.add().



private Put constructRow(String columnFamily, String driverId, String truckId,  
 Timestamp eventTime, String eventType, String latitude, String longitude)  
 {

    String rowKey = consructKey(driverId, truckId, eventTime);  
    ...  
    put.add(CF_EVENTS_TABLE, COL_DRIVER_ID, Bytes.toBytes(driverId));  
    put.add(CF_EVENTS_TABLE, COL_TRUCK_ID, Bytes.toBytes(truckId));

    ...  
}
</code></pre>

<p>This executes the getInfractionCountForDriver() to get the count of events for a driver using driverID and stores the data in HBase with constructRow() function.</p>

<pre><code>public void execute(Tuple tuple)  
 {

    ...  
    long incidentTotalCount = getInfractionCountForDriver(driverId);

    ...

        Put put = constructRow(EVENTS_TABLE_NAME, driverId, truckId, eventTime, eventType,  
                            latitude, longitude);  
        this.eventsTable.put(put);

    ...  
            incidentTotalCount = this.eventsCountTable.incrementColumnValue(Bytes.toBytes(driverId), CF_EVENTS_COUNT_TABLE,  
                                                                                           ...  
}
</code></pre>

<p>7.<strong>TruckEventProcessingTopology.java</strong></p>

<p>HDFS and HBase Bolt configurations created within configureHDFSBolt() and configureHBaseBolt() respectively. </p>

<pre><code>public void configureHDFSBolt(TopologyBuilder builder)  
{

    HdfsBolt hdfsBolt = new HdfsBolt()  
                     .withFsUrl(fsUrl)  
             .withFileNameFormat(fileNameFormat)  
             .withRecordFormat(format)  
             .withRotationPolicy(rotationPolicy)  
             .withSyncPolicy(syncPolicy)  
             .addRotationAction(hivePartitionAction);

}  
public void configureHBaseBolt(TopologyBuilder builder)  
{
    TruckHBaseBolt hbaseBolt = new TruckHBaseBolt(topologyConfig);  
    builder.setBolt(HBASE_BOLT_ID, hbaseBolt, 2).shuffleGrouping(KAFKA_SPOUT_ID);  
}
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/Ingesting%20and%20processing%20Real-time%20events%20with%20Apache%20Storm/">
        Ingesting and processing Real-time events with Apache Storm
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<p>Trucking business is a high-risk business where truck drivers venture into remote areas, often despite harsh weather conditions and chaotic traffic on a daily basis. Using this solution illustrating Modern Data Archtecture with Hortonworks Data Platform, we have developed a centralized management system that can help reduce risk and lower the total cost of operations. This system can take into consideration adverse weather conditions, the driver’s driving patterns, current traffic conditions and other criteria to alert and inform the management staff and the drivers themselves when risk factors run high.</p>

<p>In a <a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">previous tutorial</a> we learned to collect this data using Apache Kafka.</p>

<p>In this tutorial we will use <a href="http://hortonworks.com/labs/storm/"><strong>Apache Storm</strong></a> on the Hortonworks Data Platform to capture these data events and process them in real time for further analysis.</p>

<h3 id="technologies-used:a1b3bc07fb00147f3ac2e3513fe5d148">Technologies Used</h3>

<p>Hadoop, HDFS, Hive, HBase, Kafka, Storm, Falcon, Leafletjs.</p>

<h3 id="data-sets-used:a1b3bc07fb00147f3ac2e3513fe5d148">Data Sets Used</h3>

<ul>
<li>New York City Truck Routes from NYC DOT.</li>
<li>Truck Events Data generated using a custom simulator.</li>
<li>Weather Data, collected using APIs from Forcast.io.</li>
<li>Traffic Data, collected using APIs from MapQuest.</li>
</ul>

<p><em>All data sets used in these tutorials are real data sets but modified to fit these use cases.</em></p>

<h3 id="about-storm:a1b3bc07fb00147f3ac2e3513fe5d148">About Storm</h3>

<p>Apache Storm is an Open Source distributed, reliable, fault tolerant system for real time processing of data at high velocity.</p>

<p>It’s used for:</p>

<ul>
<li>Real time analytics</li>
<li>Online machine learning</li>
<li>Continuous statics computations</li>
<li>Operational Analytics</li>
<li>And, to enforce Extract, Transform, and Load (ETL) paradigms.</li>
</ul>

<p>Spout and Bolt are the two main components in Storm, which work together to process streams of data.</p>

<ul>
<li>Spout: Works on the source of data streams. In the &ldquo;Truck Events&rdquo; use case, Spout will read data from Kafka “truckevent” topics.</li>
<li>Bolt: Spout passes streams of data to Bolt which processes and passes it to either a data store or another Bolt.</li>
</ul>

<p>For details on Storm, <a href="http://hortonworks.com/labs/storm/">click here</a>.</p>

<p>In this tutorial, you will learn the following topics:</p>

<ol>
<li>Managing Storm on HDP.</li>
<li>Creating a Storm spout to consume the Kafka ‘truckevents’ generated in <a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Tutorial #1</a>.</li>
</ol>

<h2 id="prerequisites:a1b3bc07fb00147f3ac2e3513fe5d148">Prerequisites</h2>

<p><a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Tutorial #1 should be completed successfully.</a></p>

<h3 id="step-1-configure-storm:a1b3bc07fb00147f3ac2e3513fe5d148">Step 1: <strong>Configure Storm.</strong></h3>

<p>Verify if Apache Storm is installed and started by login into Ambari</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image01.png" alt="" />
</p>

<p>Note: If you do not see Storm listed under Services, please follow click on Action-&gt;Add Service and select Storm and deploy it.</p>

<h4 id="check-storm-configurations-on-the-sandbox-by-login-into-ambari:a1b3bc07fb00147f3ac2e3513fe5d148">Check Storm configurations on the Sandbox by login into Ambari.</h4>

<ul>
<li>Zookeeper configuration:<br /></li>
</ul>

<p>Ensure storm.zookeeper.servers is set to <code>sandbox.hortonworks.com</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image04.png" alt="" />
</p>

<ul>
<li>Check the local directory configuration:<br /></li>
</ul>

<p>Ensure storm.local.dir is set to <code>/hadoop/storm</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image07.png" alt="" />
</p>

<ul>
<li>Check the nimbus host configuration:<br /></li>
</ul>

<p>Ensure nimbus.host is set to <code>sandbox.hortonworks.com</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image10.png" alt="" />
</p>

<ul>
<li>Check the slots allocated:<br /></li>
</ul>

<p>Ensure supervisor.slots.ports is set to <code>[6700, 6701]</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image13.png" alt="" />
</p>

<ul>
<li>Check the UI configuration port:<br /></li>
</ul>

<p>Ensure ui.port is set to <code>8744</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image16.png" alt="" />
</p>

<h4 id="check-the-storm-ui-from-the-quick-links:a1b3bc07fb00147f3ac2e3513fe5d148">Check the Storm UI from the Quick Links</h4>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/storm2/image19.png" alt="" />
</p>

<p>Now you can see the UI:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/Storm+UI.png" alt="Storm UI" />
<br />
Storm UI</p>

<h3 id="step-2-creating-a-storm-spout-to-consume-the-kafka-truck-events-generated-in-tutorial-1:a1b3bc07fb00147f3ac2e3513fe5d148">Step 2. <strong>Creating a Storm Spout to consume the Kafka truck events generated in Tutorial #1.</strong></h3>

<h4 id="new-york-city-truck-routes:a1b3bc07fb00147f3ac2e3513fe5d148">New York City truck routes:</h4>

<p>The required <a href="http://www.nyc.gov/html/dot/downloads/misc/all_truck_routes_nyc.kml">New York City truck routes</a> KML file is included in the master.zip file. If required you can download the latest copy of the file with the following command. </p>

<pre><code>[root@sandbox ~]# wget http://www.nyc.gov/html/dot/downloads/misc/all_truck_routes_nyc.kml --directory-prefix=/opt/TruckEvents/Tutorials-master/src/main/resources/
</code></pre>

<p>Compile the code using Maven after downloading a new data file or on completing any changes to the code under <code>/opt/TruckEvents/Tutorials-master/src</code> directory.</p>

<pre><code>[root@sandbox ~]# cd /opt/TruckEvents/Tutorials-master/  
[root@sandbox ~]# export PATH=/usr/local/apache-maven-3.2.2/bin:$PATH  
[root@sandbox ~]# mvn clean package
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/mvn+clean+package.png" alt="mvn clean package" />
<br />
mvn clean package<img src="http://hortonassets.s3.amazonaws.com/storm-truck/Build+Success.png" alt="mvn build success" />
<br />
mvn build success</p>

<p>We now have a successfully compiled the code.</p>

<p>Look through the Java code under ‘src/main/java’ directory.</p>

<h4 id="verify-that-kafka-process-is-running:a1b3bc07fb00147f3ac2e3513fe5d148">Verify that Kafka process is running</h4>

<p>After successful completion of Tutorial #1, we should now have a running instance of Kafka. To verify execute ‘jps’ command to find Kafka in the running Java processes.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/Kafka+Running.png" alt="Kafka Running" />
<br />
Kafka Running</p>

<p>If Kafka is not running, you can start by login into Ambari (Follow instruction in the previous tutorial)</p>

<h4 id="loading-storm-topology:a1b3bc07fb00147f3ac2e3513fe5d148">Loading Storm Topology</h4>

<p>We now have ‘supervisor’ daemon and Kafka processes running.</p>

<p>The command below will start a new Storm Topology for TruckEvents.</p>

<pre><code>[root@sandbox ~]# [root@sandbox ~]# storm jar target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial2.TruckEventProcessingTopology
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/storm+new+topology.png" alt="storm new topology" />
<br />
storm new topology</p>

<p>Refresh the browser to see new Topology ‘truck-event-processor’ in the browser.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/truck+event+topology.png" alt="truck event processor new topology" />
<br />
truck event processor new topology</p>

<h4 id="generationg-truckevents:a1b3bc07fb00147f3ac2e3513fe5d148">Generationg TruckEvents</h4>

<p>The TruckEvents producer can now be executed as we did in Tutorial #1.</p>

<pre><code>java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer  localhost:9092 localhost:2181
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/TruckEvents+Producer.png" alt="Truck Events Producer" />
<br />
Truck Events Producer</p>

<p>Refresh the browser and you can see that messages are processed in real time by Spout.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/storm-truck/Storm+spout+messages+count.png" alt="kafkaSpout count" />
<br />
kafkaSpout count</p>

<h3 id="code-description:a1b3bc07fb00147f3ac2e3513fe5d148">Code description</h3>

<p>Let us review the code used in this tutorial. The source files are under the <code>/opt/TruckEvents/Tutorials-master/src/main/java/tutorial2</code> folder. </p>

<pre><code>[root@sandbox Tutorials-master]# ls -l src/main/java/tutorial2/  
total 16  
-rw-r--r-- 1 root root  861 Jul 24 23:34 BaseTruckEventTopology.java  
-rw-r--r-- 1 root root 1205 Jul 24 23:34 LogTruckEventsBolt.java  
-rw-r--r-- 1 root root 2777 Jul 24 23:34 TruckEventProcessingTopology.java  
-rw-r--r-- 1 root root 2233 Jul 24 23:34 TruckScheme.java  
[root@sandbox Tutorials-master]# 
</code></pre>

<h4 id="basetruckeventtopology-java:a1b3bc07fb00147f3ac2e3513fe5d148">BaseTruckEventTopology.java</h4>

<pre><code>topologyConfig.load(ClassLoader.getSystemResourceAsStream(configFileLocation));
</code></pre>

<p>Is the base class, where the topology configurations is initialized from the /resource/truck_event_topology.properties files.</p>

<h4 id="truckeventprocessingtopology-java:a1b3bc07fb00147f3ac2e3513fe5d148">TruckEventProcessingTopology.java</h4>

<p>This is the storm topology configuration class, where the Kafka spout and LogTruckevent Bolts are initialized. In the following method the Kafka spout is configured.</p>

<pre><code>private SpoutConfig constructKafkaSpoutConf()  
    {
 …  
 SpoutConfig spoutConfig = new SpoutConfig(hosts, topic, zkRoot, consumerGroupId);  
…
        spoutConfig.scheme = new SchemeAsMultiScheme(new TruckScheme());

return spoutConfig;  
    }
</code></pre>

<p>A logging bolt that prints the message from the Kafka spout was created for debugging purpose just for this tutorial.</p>

<p><code>
public void configureLogTruckEventBolt(TopologyBuilder builder)  
{  
LogTruckEventsBolt logBolt = new LogTruckEventsBolt();  
builder.setBolt(LOG_TRUCK_BOLT_ID, logBolt).globalGrouping(KAFKA_SPOUT_ID);  
}  
</code></p>

<p>The topology is built and submitted in the following method;</p>

<p><code>
private void buildAndSubmit() throws Exception  
{  
...  
StormSubmitter.submitTopology(&quot;truck-event-processor&quot;,  
conf, builder.createTopology());  
}  
</code></p>

<h4 id="truckscheme-java:a1b3bc07fb00147f3ac2e3513fe5d148">TruckScheme.java</h4>

<p>Is the deserializer provided to the kafka spout to deserialize kafka byte message stream to Values objects.</p>

<pre><code>public List&lt;Object&gt; deserialize(byte[] bytes)  
        {
        try  
                {
            String truckEvent = new String(bytes, &quot;UTF-8&quot;);  
            String[] pieces = truckEvent.split(&quot;\\|&quot;);

            Timestamp eventTime = Timestamp.valueOf(pieces[0]);  
            String truckId = pieces[1];  
            String driverId = pieces[2];  
            String eventType = pieces[3];  
            String longitude= pieces[4];  
            String latitude  = pieces[5];  
            return new Values(cleanup(driverId), cleanup(truckId),  
                                    eventTime, cleanup(eventType), cleanup(longitude), cleanup(latitude));

        }  
                catch (UnsupportedEncodingException e)  
                {
                    LOG.error(e);  
                    throw new RuntimeException(e);  
        }

    }
</code></pre>

<h4 id="logtruckeventsbolt-java:a1b3bc07fb00147f3ac2e3513fe5d148">LogTruckEventsBolt.java</h4>

<p>LogTruckEvent spout logs the kafka message received from the kafka spout to the log files under /var/log/storm/worker-*.log</p>

<p><code>
public void execute(Tuple tuple)  
{  
LOG.info(tuple.getStringByField(TruckScheme.FIELD_DRIVER_ID) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_TRUCK_ID) + &quot;,&quot; +  
tuple.getValueByField(TruckScheme.FIELD_EVENT_TIME) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_EVENT_TYPE) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_LATITUDE) + &quot;,&quot; +  
tuple.getStringByField(TruckScheme.FIELD_LONGITUDE));  
}  
</code></p>

<p>In this tutorial we have learned to capture data from Kafka Producer into Storm Spout. This data can now be processed in real time. In our next Tutorial, using Storm Bolt we will learn to store data into multiple sources to persist.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/Introducing%20Apache%20Hadoop%20to%20Developers/">
        Introducing Apache Hadoop to Developers
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<h1 id="introducing-apache-hadoop-to-developers:0a3b4c8f5f9052cb439b35757c54a7bb">Introducing Apache Hadoop to Developers</h1>

<p>Apache Hadoop is a community driven open-source project goverened by the <a href="http://apache.org/">Apache Software Foundation</a>.</p>

<p>It was originally implemented at Yahoo based on papers published by Google in 2003 and 2004. Hadoop committers today work at several different organizations like Hortonworks, Microsoft, Facebook, Cloudera and many others around the world.</p>

<p>Since then Apache Hadoop was matured and developed to become a data platform for not just processing humongous amount of data in batch but with the advent of <a href="http://hortonworks.com/labs/yarn/">YARN</a> it now supports many diverse workloads such as Interactive queries over large data with <a href="http://hortonworks.com/labs/stinger/">Hive on Tez</a>, Realtime data processing with <a href="http://hortonworks.com/labs/storm/">Apache Storm</a>, super scalable NoSQL datastore like <a href="http://hortonworks.com/hadoop/hbase/">HBase</a>, in-memory datastore like <a href="http://hortonworks.com/hadoop/spark/">Spark</a> and the list goes on.</p>

<p><img src="http://hortonworks.com/wp-content/uploads/2014/09/5-boxes.png" alt="" />
</p>

<p>For this introductory tutorial for Hadoop Developers we are going to focus on the basics, like:</p>

<ul>
<li>The core concepts of Apache Hadoop</li>
<li>Writing a MapReduce program</li>
</ul>

<p>We have many <a href="http://hortonworks.com/tutorials">tutorials</a> which you can use with the Hortonworks Sandbox to learn about a rich and diverse set of components of the Hadoop platform.</p>

<h3 id="core-of-apache-hadoop:0a3b4c8f5f9052cb439b35757c54a7bb">Core of Apache Hadoop</h3>

<ul>
<li>The Hadoop Distributed File System (HDFS)</li>
<li>MapReduce</li>
</ul>

<p>A set of machines running HDFS and MapReduce is known as a Hadoop Cluster. Individual machines are known as nodes. A cluster can have as few as one node to as many as several thousands. For most application scenarios Hadoop is linearly scalable, which means you can expect better performance by simply adding more nodes.</p>

<h3 id="mapreduce:0a3b4c8f5f9052cb439b35757c54a7bb">MapReduce</h3>

<p>MapReduce is a method for distributing a task across multiple nodes. Each node processes data stored on that node to the extent possible.</p>

<p>A running Map Reduce job consists of various phases such as <code>Map -&gt; Sort -&gt; Shuffle -&gt; Reduce</code></p>

<p>The primary advantages of abstracting your jobs as MapReduce running over a distributed infrastructure like CPU and Storage are:</p>

<ul>
<li>Automatic parallelization and distribution of data in blocks across a distributed, scale-out infrastructure.</li>
<li>Fault-tolerance against failure of storage, compute and network infrastructure</li>
<li>Deployment, monitoring and security capability</li>
<li>A clean abstraction for programmers</li>
</ul>

<p>Most MapReduce programs are written in Java. It can also be written in any scripting language using the Streaming API of Hadoop. MapReduce abstracts all the low level plumbing away from the developer such that developers can concentrate on writing the Map and Reduce functions.</p>

<h4 id="the-mapreduce-concepts-and-terminology:0a3b4c8f5f9052cb439b35757c54a7bb">The MapReduce Concepts and Terminology</h4>

<p>MapReduce jobs are controlled by a software daemon known as the <code>JobTracker</code>. The JobTracker resides on a &lsquo;master node&rsquo;. Clients submit MapReduce jobs to the JobTracker. The JobTracker assigns Map and Reduce tasks to other nodes on the cluster.</p>

<p>These nodes each run a software daemon known as the <code>TaskTracker</code>. The TaskTracker is responsible for actually instantiating the Map or Reduce task, and reporting progress back to the JobTracker</p>

<p>A <code>job</code> is a program with the ability of complete execution of Mappers and Reducers over a dataset. A <code>task</code> is the execution of a single Mapper or Reducer over a slice of data.</p>

<p>There will be at least as many task attempts as there are tasks. If a task attempt fails, another will be started by the JobTracker. Speculative execution can also result in more task attempts than completed tasks.</p>

<h4 id="mapreduce-the-mapper:0a3b4c8f5f9052cb439b35757c54a7bb">MapReduce: The Mapper</h4>

<p>Hadoop attempts to ensure that Mappers run on nodes which hold their portion of the data locally, to minimize network traffic. Multiple Mappers run in parallel, each processing a portion of the input data.</p>

<p>The Mapper reads data in the form of key/value pairs. It outputs zero or more key/value pairs</p>

<pre><code>map(in_key, in_value) -&gt; (inter_key, inter_value) list
</code></pre>

<p>The Mapper may use or completely ignore the input key. For example, a standard pattern is to read a line of a file at a time. The key is the byte offset into the file at which the line starts. The value is the contents of the line itself. Typically the key is considered irrelevant. If the Mapper writes anything out, the output must be in the form of key/value pairs.</p>

<h4 id="mapreduce-the-reducer:0a3b4c8f5f9052cb439b35757c54a7bb">MapReduce: The Reducer</h4>

<p>After the Map phase is over, all the intermediate values for a given intermediate key are combined together into a list. This list is given to a Reducer. There may be a single Reducer, or multiple Reducers, this is specified as part of the job configuration. All values associated with a particular intermediate key are guaranteed to go to the same Reducer.</p>

<p>The intermediate keys, and their value lists, are passed to the Reducer in sorted key order. This step is known as the &lsquo;shuffle and sort&rsquo;. The Reducer outputs zero or more final key/value pairs. These are written to HDFS. In practice, the Reducer usually emits a single key/value pair for each input key.</p>

<p>It is possible for some Map tasks to take more time to complete than the others, often due to faulty hardware, or underpowered machines. This might cause a bottleneck as all mappers need to finish before any reducers can kick-off. Hadoop uses speculative execution to mitigate against such situations. If a Mapper appears to be running sluggishly than the others, a new instance of the Mapper will be started on another machine, operating on the same data. The results of the first Mapper to finish will be used. Hadoop will kill off the Mapper which is still running.</p>

<h3 id="writing-a-mapreduce-program:0a3b4c8f5f9052cb439b35757c54a7bb">Writing a MapReduce Program</h3>

<p>In this section you will learn how to use the Hadoop API to write a MapReduce program in Java</p>

<p>Each of the portions (RecordReader, Mapper, Partitioner, Reducer, etc.) can be created by the developer. The developer is expected to atleast write the Mapper, Reducer, and driver code.</p>

<h4 id="the-mapreduce-example:0a3b4c8f5f9052cb439b35757c54a7bb">The MapReduce Example</h4>

<p><strong>WordCount</strong> example reads text files and counts how often words occur. The input is text files and the output is text files, each line of which contains a word and the count of how often it occured, separated by a tab.</p>

<p>Each mapper takes a line as input and breaks it into words. It then emits a key/value pair of the word and 1. Each reducer sums the counts for each word and emits a single key/value with the word and sum.</p>

<p>As an optimization, the reducer is also used as a combiner on the map outputs. This reduces the amount of data sent across the network by combining each word into a single record.</p>

<p>To run the example, the command syntax is</p>

<pre><code>hadoop jar hadoop-*-examples.jar wordcount [-m &lt;#maps&gt;] [-r &lt;#reducers&gt;] &lt;in-dir&gt; &lt;out-dir&gt;
</code></pre>

<p>All of the files in the input directory are read and the counts of words in the input are written to the output directory. It is assumed that both inputs and outputs are stored in HDFS. If your input is not already in HDFS, but is rather in a local file system somewhere, you need to copy the data into HDFS using a command like this:</p>

<pre><code>hadoop dfs -copyFromLocal &lt;local-dir&gt; &lt;hdfs-dir&gt;
</code></pre>

<p>Below is the standard wordcount example implemented in Java:</p>

<pre><code>    package org.myorg;

    import java.io.IOException;
    import java.util.*;

    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.mapreduce.*;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

    public class WordCount {

     public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                context.write(word, one);
            }
        }
     } 

     public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) 
          throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
     }

     public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();

            Job job = new Job(conf, &quot;wordcount&quot;);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);

        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.waitForCompletion(true);
     }

    }
</code></pre>

<p>Every MapReduce job consists of three portions</p>

<ul>
<li>The driver code</li>
<li>Code that runs on the client to configure and submit the job</li>
<li>The Mapper</li>
<li>The Reducer</li>
</ul>

<p>Before we look at the code, we need to cover some basic Hadoop API concepts</p>

<h4 id="mapper-reading-data-from-hdfs:0a3b4c8f5f9052cb439b35757c54a7bb">Mapper reading data from HDFS</h4>

<p>The data passed to the Mapper is specified by an InputFormat. The InputFormat is specified in the driver code. It defines the location of the input data like a file or directory on HDFS. It also determines how to split the input data into input splits.</p>

<p>Each Mapper deals with a single input split. InputFormat is a factory for RecordReader objects to extract (key, value) records from the input source.</p>

<p>FilelnputFormat is the base class used for all file-based InputFormats. TextlnputFormat is the default FilelnputFormat. It treats each \n-terminated line of a file as a value. The Key is the byte offset within the file of that line. KeyValueTextlnputFormat maps \n-terminated lines as &lsquo;key SEP value&rsquo;. By default, separator is a tab. SequenceFilelnputFormat is a binary file of (key, value) pairs with some additional metadata. SequenceFileAsTextlnputFormat is similar, but maps (key.toString( ), value.toString( )).</p>

<p>Keys and values in Hadoop are objects. Values are objects which implement the writable interface. Keys are objects which implement writableComparable.</p>

<h4 id="writable:0a3b4c8f5f9052cb439b35757c54a7bb">Writable</h4>

<p>Hadoop defines its own &lsquo;box classes&rsquo; for strings, integers and so on:</p>

<ul>
<li>IntWritable for ints</li>
<li>LongWritable for longs</li>
<li>FloatWritable for floats</li>
<li>DoubleWritable for doubles</li>
<li>Text for strings</li>
<li>Etc.</li>
</ul>

<p>The writable interface makes serialization quick and easy for Hadoop. Any value&rsquo;s type must implement the writable interface.</p>

<h4 id="writablecomparable:0a3b4c8f5f9052cb439b35757c54a7bb">WritableComparable</h4>

<p>A WritableComparable is a Writable which is also Comparable. Two writableComparables can be compared against each other to determine their &lsquo;order&rsquo;. Keys must be WritableComparables because they are passed to the Reducer in sorted order.</p>

<p>Note that despite their names, all Hadoop box classes implement both Writable and WritableComparable, for example, intwritable is actually a WritableComparable</p>

<h4 id="driver:0a3b4c8f5f9052cb439b35757c54a7bb">Driver</h4>

<p>The driver code runs on the client machine. It configures the job, then submits it to the cluster.</p>

<h3 id="streaming-api:0a3b4c8f5f9052cb439b35757c54a7bb">Streaming API</h3>

<p>Many organizations have developers skilled in languages other than Java, such as</p>

<ul>
<li>C#</li>
<li>Ruby</li>
<li>Python</li>
<li>Perl</li>
</ul>

<p>The Streaming API allows developers to use any language they wish to write Mappers and Reducers as long as the language can read from standard input and write to standard output.</p>

<p>The advantages of the Streaming API are that there is no need for non-Java coders to learn Java. So it results in faster development time and the ability to use existing code libraries.</p>

<h4 id="how-streaming-works:0a3b4c8f5f9052cb439b35757c54a7bb">How Streaming Works</h4>

<p>To implement streaming, write separate Mapper and Reducer programs in the language of your choice. They will receive input via stdin. They should write their output to stdout.</p>

<p>If TextinputFormat (the default) is used, the streaming Mapper just receives each line from the file on stdin where no key is passed. Streaming Mapper and streaming Reducer&rsquo;s output should be sent to stdout as key (tab) value (newline) and the Separators other than tab can be specified.</p>

<p>In Java, all the values associated with a key are passed to the Reducer as an iterator. Using Hadoop Streaming, the Reducer receives its input as (key, value) pairs, one per line of standard input.</p>

<p>Your code will have to keep track of the key so that it can detect when values from a new key start appearing launching a Streaming Job .To launch a Streaming job, use e.g.,:</p>

<pre><code>hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming*.jar \
-input mylnputDirs \ -output myOutputDir \
-mapper myMap.py \
-reducer myReduce.py \ -file myMap.py \ -file myReduce.py
</code></pre>

<h3 id="hive-and-pig-motivation:0a3b4c8f5f9052cb439b35757c54a7bb">Hive and Pig: Motivation</h3>

<p>MapReduce code is typically written in Java. Although it can be written in other languages using Hadoop</p>

<p>Streaming Requires a programmer who understands how to think in terms of MapReduce, who understands the problem they&rsquo;re trying to solve and who has enough time to write and test the code.</p>

<p>Many organizations have only a few developers who can write good MapReduce code</p>

<p>Meanwhile, many other people want to analyze data</p>

<ul>
<li>Data analysts</li>
<li>Business analysts</li>
<li>Data scientists</li>
<li>Statisticians</li>
</ul>

<p>So we needed a higher-level abstraction on top of MapReduce providing the ability to query the data without needing to know MapReduce intimately. Hive and Pig address these needs.</p>

<p>See the following tutorial for more on Hive and Pig:</p>

<ul>
<li><a href="http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/">Process Data with Apache Hive</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-pig/">Process Data with Apache Pig</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/cascading-log-parsing/">Get Started with Cascading on Hortonworks Data Platform</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/supercharging-interactive-queries-hive-tez/">Interactive Query for Hadoop with Apache Hive on Apache Tez</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/exploring-data-apache-pig-grunt-shell/">Exploring Data with Apache Pig from the Grunt shell</a></li>
</ul>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/How%20To%20Process%20Data%20with%20Apache%20Pig/">
        How To Process Data with Apache Pig
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<h3 id="what-is-pig:2a7125070cad15a430fa7d68d009d8bd">What is Pig?</h3>

<p>Pig is a high level scripting language that is used with Apache Hadoop. Pig excels at describing data analysis problems as data flows. Pig is complete in that you can do all the required data manipulations in Apache Hadoop with Pig. In addition through the User Defined Functions(UDF) facility in Pig you can have Pig invoke code in many languages like JRuby, Jython and Java. Conversely you can execute Pig scripts in other languages. The result is that you can use Pig as a component to build larger and more complex applications that tackle real business problems.</p>

<p>A good example of a Pig application is the ETL transaction model that describes how a process will extract data from a source, transform it according to a rule set and then load it into a datastore. Pig can ingest data from files, streams or other sources using the User Defined Functions(UDF). Once it has the data it can perform select, iteration, and other transforms over the data. Again the UDF feature allows passing the data to more complex algorithms for the transform. Finally Pig can store the results into the Hadoop Data File System.</p>

<p>Pig scripts are translated into a series of MapReduce jobs that are run on the Apache Hadoop cluster. As part of the translation the Pig interpreter does perform optimizations to speed execution on Apache Hadoop. We are going to write a Pig script that will do our data analysis task.</p>

<h3 id="our-data-processing-task:2a7125070cad15a430fa7d68d009d8bd">Our data processing task</h3>

<p>We are going to read in a baseball statistics file. We are going to compute the highest runs by a player for each year. This file has all the statistics from 1871-2011 and it contains over 90,000 rows. Once we have the highest runs we will extend the script to translate a player id field into the first and last names of the players.</p>

<h3 id="downloading-the-data:2a7125070cad15a430fa7d68d009d8bd">Downloading the data</h3>

<p>The data file we are using comes from the site www.seanlahman.com. You can download the data file in csv zip form from:</p>

<p><a href="http://hortonassets.s3.amazonaws.com/pig/lahman591-csv.zip">http://hortonassets.s3.amazonaws.com/pig/lahman591-csv.zip</a></p>

<p>Once you have the file you will need to unzip the file into a directory. We will be uploading just the master.csv and batting.csv files.</p>

<h3 id="uploading-the-data-files:2a7125070cad15a430fa7d68d009d8bd">Uploading the data files</h3>

<p>We start by selecting the File Browser from the top tool bar. The File Browser allows us to view the Hortonworks Data Platform(HDP) file store. This is separate from the local file system. In a Hadoop cluster this would be your view of the Hadoop Data File System(HDFS). For the Hortonworks Sandbox it will be part of the file system in the Hortonworks Sandbox VM.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/1FileBrowser.jpg?raw=true" alt="" />
</p>

<p>Click on the Upload button to select the files we want to upload into the Hortonworks Sandbox environment.</p>

<p>When you click on the Upload a file button you will get a dialog box. Navigate to where you stored the Batting.csv file on your local disk and select Batting.csv. Do the same thing for Master.csv. When you are done you will see there are two files in your directory.</p>

<p>Now that we have our data files we can start writing our Pig script. Click on the Pig icon at the top of the screen.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/5PigIcon.jpg?raw=true" alt="" />
/p&gt;</p>

<p>We see the Pig user interface in our browser window. On the left is a list of the saved scripts. On the right is the composition area where we will be writing our script. Below the composition area are buttons to Save, Execute, Explain and perform a Syntax check of the current script. At the very bottom are status boxes where we will see logs, error message and the output of our script.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/6PigUI.jpg?raw=true" alt="" />
</p>

<p>To get started fill in a name for your script. You can not save it until we add our first line of code. The first thing we need to do is load the data. We use the load statement for this. The PigStorage function is what does the loading and we pass it a comma as the data delimiter. Our code is:</p>

<pre><code>batting = load 'Batting.csv' using PigStorage(',');
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/7LoadStmt.jpg?raw=true" alt="" />
</p>

<p>The next thing we want to do is name the fields. We will use a FOREACH statement to iterate through the batting data object. We can use Pig Helper that is at the bottom of the composition area to provide us with a template. We will click on Pig Helper, select Data processing functions and then click on the FOREACH template. We can then replace each element by hitting the tab key.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/8PigHelperForeach.jpg?raw=true" alt="" />
</p>

<p>So the FOREACH statement will iterate through the batting data object and GENERATE pulls out selected fields and assigns them names. The new data object we are creating is then named runs. Our code will now be:</p>

<pre><code>runs = FOREACH batting GENERATE $0 as playerID, $1 as year, $8 as runs;
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/9ForeachStmt.jpg?raw=true" alt="" />
</p>

<p>The next line of code is a group statement that groups the elements in runs by the year field. So the grp_data object will then be indexed by year. In the next statement as we iterate through grp_data we will go through year by year. Type in the code:</p>

<pre><code>grp_data = GROUP runs by (year);
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/10GroupBy.jpg?raw=true" alt="" />
</p>

<p>In the next FOREACH statement we are going to find the maximum runs for each year. The code for this is:</p>

<pre><code>max_runs = FOREACH grp_data GENERATE group as grp,MAX(runs.runs) as max_runs;
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/11ForeachMax.jpg?raw=true" alt="" />
</p>

<p>Now that we have the maximum runs we need to join this with the runs data object so we can pick up the player id. The result will be a dataset with Year, PlayerID and Max Run. At the end we dump the data to the output.</p>

<pre><code>join_max_run = JOIN max_runs by ($0, max_runs), runs by (year,runs);
join_data = FOREACH join_max_run GENERATE $0 as year, $2 as playerID, $1 as runs;
dump join_data;
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/12Join.jpg?raw=true" alt="" />
</p>

<p>Let’s take a look at our script. The first thing to notice is we never really address single rows of data to the left of the equals sign and on the right we just describe what we want to do for each row. We just assume things are applied to all the rows. We also have powerful operators like GROUP and JOIN to sort rows by a key and to build new data objects.</p>

<p>At this point we can save our script. Fill in a name in the box below “Pig script:” if you haven’t already. Click on the save button and the your script will show up in the bar on the left. <img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/13aSaveScript.jpg?raw=true" alt="" />
</p>

<p>We can execute our code by clicking on the execute button at the bottom of the composition area. As the jobs are run you will get a progress bar at the bottom.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/13ProgressBar.jpg?raw=true" alt="" />
</p>

<p>When the job completes the results are displaying in the green box at the bottom.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/14Results.jpg?raw=true" alt="" />
</p>

<p>If you scroll down to the “Logs…” and click on the link you can see the log file of your jobs.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-2/15Logs.jpg?raw=true" alt="" />
</p>

<p>So we have created a simple Pig script that reads in some comma separated data. Once we have that set of records in Pig we pull out the playerID, year and runs fields from each row. We them sort them by year with one statement, GROUP. Then for each year we find the maximum runs. This is finally mapped to the playerID and we produce our final dataset.</p>

<p>As mentioned before Pig operates on data flows. We consider each group of rows together and we specify how we operate on them as a group. As the datasets get larger and/or add fields our Pig script will remain pretty much the same because it is concentrating on how we want to manipulate the data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/How%20To%20Process%20Data%20with%20Apache%20Hive/">
        How To Process Data with Apache Hive
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<p>This tutorial was derived from material in the Hortonworks developer training. These classes cover uses of the tools in the Hortonworks Data Platform and how to develop applications and projects using the Hortonworks Data Platform. You can find more information about this subject at <a href="http://hortonworks.com/training/class/hadoop-2-data-analysis-pig-hive/">Data Analysis with Pig &amp; Hive</a></p>

<h3 id="data-processing-with-hive:e2779fd644b3b0cf69f3c64377d1a090">Data processing with Hive</h3>

<p>Hive is a component of Hortonworks Data Platform(HDP). Hive provides a SQL-like interface to data stored in HDP. In the previous tutorial we used Pig which is a scripting language with a focus on dataflows. Hive provides a database query interface to Apache Hadoop.</p>

<p>People often ask why do Pig and Hive exist when they seem to do much of the same thing. Hive because of its SQL like query language is often used as the interface to an Apache Hadoop based data warehouse. Hive is considered friendlier and more familiar to users who are used to using SQL for querying data. Pig fits in through its data flow strengths where it takes on the tasks of bringing data into Apache Hadoop and working with it to get it into the form for querying. A good overview of how this works is in Alan Gates posting on the Yahoo Developer blog titled <a href="https://developer.yahoo.com/blogs/hadoop/pig-hive-yahoo-464.html">Pig and Hive at Yahoo!</a> From a technical point of view both Pig and Hive are feature complete so you can do tasks in either tool. However you will find one tool or the other will be preferred by the different groups that have to use Apache Hadoop. The good part is they have a choice and both tools work together.</p>

<h3 id="our-data-processing-task:e2779fd644b3b0cf69f3c64377d1a090">Our data processing task</h3>

<p>We are going to do the same data processing task as we just did with Pig in the previous tutorial. We have several files of baseball statistics and we are going to bring them into Hive and do some simple computing with them. We are going to find the player with the highest runs for each year. This file has all the statistics from 1871-2011 and contains more that 90,000 rows. Once we have the highest runs we will extend the script to translate a player id field into the first and last names of the players.</p>

<h3 id="downloading-the-data:e2779fd644b3b0cf69f3c64377d1a090">Downloading the data</h3>

<p>The data files we are using comes from the site www.seanlahman.com. You can download the data file from:</p>

<p><a href="http://seanlahman.com/files/database/lahman591-csv.zip">http://seanlahman.com/files/database/lahman591-csv.zip</a></p>

<p>Once you have the file you will need to unzip it into a directory. We will be uploading just the Master.csv and Batting.csv files from the dataset.</p>

<h3 id="uploading-the-data-files:e2779fd644b3b0cf69f3c64377d1a090">Uploading the data files</h3>

<p>We start by selecting the File Browser from the top tool bar. The File Browser shows you the files in the HDP file store. In this case the file store resides in the Hortonworks Sandbox VM.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/1FileBrowser.jpg?raw=true" alt="" />
</p>

<p>Click on the Upload button</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/2UploadButton.jpg?raw=true" alt="" />
</p>

<p>You want to select Files. Then you will get a dialog box.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/3UploadBox.jpg?raw=true" alt="" />
</p>

<p>When you click on the Upload a file button you will get a dialog box. Navigate to where you stored the Batting.csv file on your local disk and select Batting.csv. Do the same thing for Master.csv. When you are done you will see there are two files in your directory.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/4LoadedFiles.jpg?raw=true" alt="" />
</p>

<h3 id="starting-beeswax-the-hive-ui:e2779fd644b3b0cf69f3c64377d1a090">Starting Beeswax, the Hive UI</h3>

<p>Lets start Beeswax by clicking on the bee icon in the top bar. Beeswax is a user interface to the Hive data warehouse system for Hadoop.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/5StartBeeswax.jpg?raw=true" alt="" />
</p>

<p>Beeswax provides a GUI to Hive. On right is a query editor. There is a limit of one query per execute cycle. A query may span multiple lines. At the bottom there are buttons to Execute the query, Save the query with a name, Explain the query and to start a new query.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/5_1QueryEditor.jpg?raw=true" alt="" />
</p>

<p>Before we get started let’s take a look at how Pig and Hive data models differ. In the case of Pig all data objects exist and are operated on in the script. Once the script is complete all data objects are deleted unless you stored them. In the case of Hive we are operating on the Apache Hadoop data store. Any query you make, table that you create, data that you copy persists from query to query. You can think of Hive as providing a data workbench where you can examine, modify and manipulate the data in Apache Hadoop. So when we perform our data processing task we will execute it one query or line at a time. Once a line successfully executes you can look at the data objects to verify if the last operation did what you expected. All your data is live, compared to Pig, where data objects only exist inside the script unless they are copied out to storage. This kind of flexibility is Hive’s strength. You can solve problems bit by bit and change your mind on what to do next depending on what you find.</p>

<p>The first task we will do is create a table to hold the data. We will type the query into the composition area on the right like this. Once you have typed in the query hit the Execute button at the bottom.</p>

<pre><code>create table temp_batting (col_value STRING);
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/7CreateTable.jpg?raw=true" alt="" />
</p>

<p>The query returns “No data available in the table” because at this point we just created an empty table and we have not copied any data in it.</p>

<p>Once the query has executed we can click on Tables at the top of the composition area and we will see we have a new table called temp_batting.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/8Tables.jpg?raw=true" alt="" />
</p>

<p>Clicking on the Browse Data button will let us see the data and right now the table is empty. This is a good example of the interactive feel you get with using Hive.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/9EmptyTable.jpg?raw=true" alt="" />
</p>

<p>The next line of code will load the data file Batting.csv into the table temp_batting. We can start typing the code and we will notice there is a helper feature that helps us fill in the correct path to our file.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/10Helper.jpg?raw=true" alt="" />
</p>

<p>The complete query looks like this.</p>

<pre><code>LOAD DATA INPATH '/user/hue/Batting.csv' OVERWRITE INTO TABLE temp_batting;
</code></pre>

<p>After executing the query we can look at the Tables again and when we browse the data for temp_batting we see that the data has been read in. Note Hive consumed the data file Batting.csv during this step. If you look in the File Browser you will see Batting.csv is no longer there.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/12Data.jpg?raw=true" alt="" />
</p>

<p>Now that we have read the data in we can start working with it. The next thing we want to do extract the data. So first we will type in a query to create a new table called batting to hold the data. That table will have three columns for player_id, year and the number of runs.</p>

<pre><code>create table batting (player_id STRING, year INT, runs INT);
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/13Createbatting.jpg?raw=true" alt="" />
</p>

<p>Then we extract the data we want from temp_batting and copy it into batting. We will do this with a regexp pattern. To do this we are going to build up a multi-line query. The first line of the query create the table batting. The three regexp_extract calls are going to extract the player_id, year and run fields from the table temp_batting. When you are done typing the query it will look like this. Be careful as there are no spaces in the regular expression pattern.</p>

<pre><code>insert overwrite table batting
SELECT
  regexp_extract(col_value, '^(?:([^,]*)\,?){1}', 1) player_id,
  regexp_extract(col_value, '^(?:([^,]*)\,?){2}', 1) year,
  regexp_extract(col_value, '^(?:([^,]*)\,?){9}', 1) run
from temp_batting;
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/14Extract.jpg?raw=true" alt="" />
</p>

<p>Execute the query and look at the batting table. You should see data that looks like this.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/15battingData.jpg?raw=true" alt="" />
</p>

<p>Now we have the data fields we want. The next step is to group the data by year so we can find the highest score for each year. This query first groups all the records by year and then selects the player with the highest runs from each year.</p>

<pre><code>SELECT year, max(runs) FROM batting GROUP BY year;
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/17SelectMaxYr.jpg?raw=true" alt="" />
</p>

<p>The results of the query look like this.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/18ResultsMaxYr.jpg?raw=true" alt="" />
</p>

<p>Now we need to go back and get the player_id(s) so we know who the player(s) was. We know that for a given year we can use the runs to find the player(s) for that year. So we can take the previous query and join it with the batting records to get the final table.</p>

<pre><code>SELECT a.year, a.player_id, a.runs from batting a
JOIN (SELECT year, max(runs) runs FROM batting GROUP BY year ) b
ON (a.year = b.year AND a.runs = b.runs) ;
</code></pre>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/19SelectJoin.jpg?raw=true" alt="" />
</p>

<p>The resulting data looks like:</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-3/20SelectJoinResults.jpg?raw=true" alt="" />
</p>

<p>This query may take a couple of minutes. While you’re waiting, if you have internet access, take a look at this video from Alan Gates to hear about the future of HCatalog:<br />
<a href="http://www.youtube.com/watch?v=gTwhSAEEe1I">Future of HCatalog</a></p>

<p>So now we have our results. As described earlier we solved this problem using Hive step by step. At any time we were free to look around at the data, decide we needed to do another task and come back. At all times the data is live and accessible to us.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/Faster%20Pig%20with%20Tez/">
        Faster Pig with Tez
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<h3 id="what-is-pig:8f9c2da6dd114e1acd205d858c272182">What is Pig?</h3>

<p>Pig is a high level scripting language that is used with Apache Hadoop. Pig excels at describing data analysis problems as data flows. Pig is complete in that you can do all the required data manipulations in Apache Hadoop with Pig. In addition through the User Defined Functions(UDF) facility in Pig you can have Pig invoke code in many languages like JRuby, Jython and Java. Conversely you can execute Pig scripts in other languages. The result is that you can use Pig as a component to build larger and more complex applications that tackle real business problems.</p>

<p>A good example of a Pig application is the ETL transaction model that describes how a process will extract data from a source, transform it according to a rule set and then load it into a datastore. Pig can ingest data from files, streams or other sources using the User Defined Functions(UDF). Once it has the data it can perform select, iteration, and other transforms over the data. Again the UDF feature allows passing the data to more complex algorithms for the transform. Finally Pig can store the results into the Hadoop Data File System.</p>

<p>Pig scripts are translated into a series of MapReduce jobs or a Tez DAG that are run on the Apache Hadoop cluster. As part of the translation the Pig interpreter does perform optimizations to speed execution on Apache Hadoop. We are going to write a Pig script that will do our data analysis task.</p>

<h3 id="what-is-tez:8f9c2da6dd114e1acd205d858c272182">What is Tez?</h3>

<p>Tez – Hindi for “speed” provides a general-purpose, highly customizable framework that creates simplifies data-processing tasks across both small scale (low-latency) and large-scale (high throughput) workloads in Hadoop. It generalizes the <a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce paradigm</a> to a more powerful framework by providing the ability to execute a complex DAG (<a href="http://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graph</a>) of tasks for a single job so that projects in the Apache Hadoop ecosystem such as Apache Hive, Apache Pig and Cascading can meet requirements for human-interactive response times and extreme throughput at petabyte scale (clearly MapReduce has been a key driver in achieving this).</p>

<h3 id="our-data-processing-task:8f9c2da6dd114e1acd205d858c272182">Our data processing task</h3>

<p>We are going to read in a baseball statistics file. We are going to compute the highest runs by a player for each year. This file has all the statistics from 1871-2011 and it contains over 90,000 rows. Once we have the highest runs we will extend the script to translate a player id field into the first and last names of the players.</p>

<h3 id="downloading-the-data:8f9c2da6dd114e1acd205d858c272182">Downloading the data</h3>

<p>The data file we are using comes from the site <a href="http://www.seanlahman.com/">www.seanlahman.com</a>. We will SSH into the VM.</p>

<p><code>ssh root@127.0.0.1 -p 2222;</code></p>

<p>the password is <code>hadoop</code></p>

<p>In case you are not running the Sandbox using VirtualBox, you may have to replace the 127.0.0.1 IP address with the actual IP of the VM.</p>

<p>You can download the data file in csv zip using the command below:</p>

<p><code>wget http://hortonassets.s3.amazonaws.com/pig/lahman591-csv.zip</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/1.png" alt="" />
</p>

<p>After the file gets downloaded, <code>unzip lahman591-csv.zip</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/2.png" alt="" />
</p>

<h3 id="uploading-into-hdfs:8f9c2da6dd114e1acd205d858c272182">Uploading into HDFS</h3>

<p>Let&rsquo;s change into the directory with <code>cd lahman591-csv</code> and use the list command <code>ls</code> to check all the files we downloaded:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/3.png" alt="" />
</p>

<p>We are going to upload the <code>Batting.csv</code> file using the command <code>hadoop fs -put ./Batting.csv /user/guest/</code>:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/4.png" alt="" />
</p>

<p>Let&rsquo;s check if the files are on HDFS, with the command <code>hadoop fs -ls /user/guest/</code>:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/5.png" alt="" />
</p>

<h3 id="running-pig-on-mapreduce:8f9c2da6dd114e1acd205d858c272182">Running Pig on MapReduce</h3>

<p>We will run first Pig without Tez.</p>

<p>So, first let&rsquo;s create the pig script with the command <code>vi 1.pig</code>:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/6.png" alt="" />
</p>

<p>Press <code>i</code> in vi to enable the insert mode. Then copy the pig script below and paste it in vi:</p>

<pre><code>batting = LOAD '/user/guest/Batting.csv' USING PigStorage(',');
raw_runs = FILTER batting BY $1&gt;0;
runs = FOREACH raw_runs GENERATE $0 AS playerID, $1 AS year, $8 AS runs;
grp_data = GROUP runs BY (year);
max_runs = FOREACH grp_data GENERATE group as grp, MAX(runs.runs) AS max_runs;
join_max_runs = JOIN max_runs BY ($0, max_runs), runs BY (year, runs);
join_data = FOREACH join_max_runs GENERATE $0 AS year, $2 AS playerID, $1 AS runs;
DUMP join_data;
</code></pre>

<p>Then hit <code>esc</code> button and type <code>:wq</code> to save the file:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/7.png" alt="" />
</p>

<p>Now we can execute the pig script using the MapReduce engine by simply typing <code>pig 1.pig</code>:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/8.png" alt="" />
</p>

<p>It typically takes a little more than two minutes to finish on our single node pseudocluster. Note the time it took on our machine after it completes:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/9.png" alt="" />
</p>

<h3 id="running-pig-on-tez:8f9c2da6dd114e1acd205d858c272182">Running Pig on Tez</h3>

<p>Let&rsquo;s run the same Pig script with Tez using the command <code>pig -x tez 1.pig</code>:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/10.png" alt="" />
</p>

<p>This time note the time after the execution completes:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/pig-tez/11.png" alt="" />
</p>

<p>On our machine it took around 58 seconds with Pig using the Tez engine. That is more than 2X faster than Pig using MapReduce even without any specific optimization in the script for Tez.</p>

<p>Tez definitely lives up to it&rsquo;s name.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/Hello%20World%20-%20An%20introduction%20to%20Hadoop%20with%20Hive%20and%20Pig/">
        Hello World
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    

<h1 id="hello-world-an-introduction-to-hadoop-with-hive-and-pig:c4646bf119986fba0efcf7fe4daeae11">Hello World! – An introduction to Hadoop with Hive and Pig</h1>

<p>The tutorials are presented in sections as listed below.</p>

<ul>
<li><a href="http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#overview">Overview of Apache Hadoop and Hortonworks Data Platform</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#usingHDP">Using HDP</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#loading">Loading the sample data into HCatalog</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#hive">A Short Apache Hive Tutorial</a></li>
<li><a href="http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#pig">Pig Basics Tutorial</a></li>
</ul>

<h3 id="overview-of-apache-hadoop-and-hortonworks-data-platform:c4646bf119986fba0efcf7fe4daeae11">Overview of Apache Hadoop and Hortonworks Data Platform</h3>

<p>The Hortonworks Sandbox is a single node implementation of the Hortonworks Data Platform(HDP). It is packaged as a virtual machine to make evaluation and experimentation with HDP fast and easy. The tutorials and features in the Sandbox are oriented towards exploring how HDP can help you solve your business big data problems. The Sandbox tutorials will walk you through bringing some sample data into HDP and manipulate it using the tools built into HDP. The idea is to show you how you can get started and show you how to accomplish tasks in HDP. HDP is free to download and use in your enterprise and you can download it here: <a href="http://hortonworks.com/download/">Hortonworks Data Platform<br />
Download</a></p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/yahoo_data_nodes-a.png?raw=true" alt="Yahoo Data Nodes" />
</p>

<p>The Apache Hadoop projects provide a series of tools designed to solve big data problems. The Hadoop cluster implements a parallel computing cluster using inexpensive commodity hardware. The cluster is partitioned across many servers to provide a near linear scalability. The philosophy of the cluster design is to bring the computing to the data. So each datanode will hold part of the overall data and be able to process the data that it holds. The overall framework for the processing software is called MapReduce. Here&rsquo;s a short video introduction to MapReduce: <a href="http://www.youtube.com/watch?v=ht3dNvdNDzI">Introduction to MapReduce</a></p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/yahoo_map_reduce-a.png?raw=true" alt="Yahoo Map Reduce" />
</p>

<p>Apache Hadoop can be useful across a range of use cases spanning virtually every vertical industry. It is becoming popular anywhere that you need to store, process, and analyze large volumes of data. Examples include digital marketing automation, fraud detection and prevention, social network and relationship analysis, predictive modeling for new drugs, retail in-store behavior analysis, and mobile device location-based marketing.</p>

<hr />

<h3 id="the-hadoop-distributed-file-system:c4646bf119986fba0efcf7fe4daeae11">The Hadoop Distributed File System</h3>

<p>In this section we are going to take a closer look at some of the components we will be using in the Sandbox tutorials. Underlying all of these components is the Hadoop Distributed File System(HDFS™). This is the foundation of the Hadoop cluster. The HDFS file system manages how the datasets are stored in the Hadoop cluster. It is responsible for distributing the data across the datanodes, managing replication for redundancy and administrative tasks like adding, removing and recovery of datanodes.</p>

<h3 id="apache-hive:c4646bf119986fba0efcf7fe4daeae11">Apache Hive™</h3>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/hive_ui_overview-b.jpg?raw=true" alt="Hive UI Overview" />
</p>

<p>The Apache Hive project provides a data warehouse view of the data in HDFS. Using a SQL-like language Hive lets you create summarizations of your data, perform ad-hoc queries, and analysis of large datasets in the Hadoop cluster. The overall approach with Hive is to project a table structure on the dataset and then manipulate it with HiveQL. Since you are using data in HDFS your operations can be scaled across all the datanodes and you can manipulate huge datasets.</p>

<h3 id="apache-hcatalog:c4646bf119986fba0efcf7fe4daeae11">Apache HCatalog</h3>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/hcat_ui_overview-a.jpg?raw=true" alt="HCat UI Overview" />
</p>

<p>The function of HCatalog is to hold location and metadata about the data in a Hadoop cluster. This allows scripts and MapReduce jobs to be decoupled from data location and metadata like the schema. Additionally since HCatalog supports many tools, like Hive and Pig, the location and metadata can be shared between tools. Using the open APIs of HCatalog other tools like Teradata Aster can also use the location and metadata in HCatalog. In the tutorials we will see how we can now reference data by name and we can inherit the location and metadata.</p>

<h3 id="apache-pig:c4646bf119986fba0efcf7fe4daeae11">Apache Pig™</h3>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/pig_ui_overview-a.jpg?raw=true" alt="Pig UI Overview" />
</p>

<p>Pig is a language for expressing data analysis and infrastructure processes. Pig is translated into a series of MapReduce jobs that are run by the Hadoop cluster. Pig is extensible through user-defined functions that can be written in Java and other languages. Pig scripts provide a high level language to create the MapReduce jobs needed to process data in a Hadoop cluster.</p>

<p>That‘s all for now… let‘s get started with some examples of using these tools together to solve real problems!</p>

<h3 id="using-hdp:c4646bf119986fba0efcf7fe4daeae11">Using HDP</h3>

<p>Here we go! We&rsquo;re going to walk you through a series of step-by-step tutorials to get you up and running with the Hortonworks Data Platform(HDP).</p>

<h3 id="downloading-example-data:c4646bf119986fba0efcf7fe4daeae11">Downloading Example Data</h3>

<p>We&rsquo;ll need some example data for our lessons. For our first lesson, we&rsquo;ll be using stock ticker data from the New York Stock Exchange from the years 2000-2001. You can download this file here:</p>

<p><a href="https://s3.amazonaws.com/hw-sandbox/tutorial1/NYSE-2000-2001.tsv.gz">https://s3.amazonaws.com/hw-sandbox/tutorial1/NYSE-2000-2001.tsv.gz</a></p>

<p>The file is about 11 megabytes, and may take a few minutes to download. Fortunately, to learn &lsquo;Big Data&rsquo; you don&rsquo;t have to use a massive dataset. You need only use tools that scale to massive datasets. Click and save this file to your computer.</p>

<h3 id="using-the-file-browser:c4646bf119986fba0efcf7fe4daeae11">Using the File Browser</h3>

<p>To access Hue, go to <a href="http://127.0.0.1:8000">http://127.0.0.1:8000</a> with your browser. Use the following username and password
  User: Hue; Password: 1111</p>

<p>You can reach the File Browser by clicking its icon:</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/pick_file_browser-a.jpg?raw=true" alt="Pick File Browser" />
</p>

<p>The File Browser interface should be familiar to you as it is similar to the file manager on a Windows PC or Mac. We begin in our home directory. This is where we&rsquo;ll store the results of our work. File Browser also lets us upload files.</p>

<h3 id="uploading-a-file:c4646bf119986fba0efcf7fe4daeae11">Uploading a File</h3>

<p>To upload the example data you just downloaded,</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/select_file_upload.jpg?raw=true" alt="Select File Upload" />
</p>

<ul>
<li>Select the &lsquo;Upload&rsquo; button</li>
<li>Select &lsquo;Files&rsquo; and a pop-up window will appear.</li>
<li>Click the button which says, &lsquo;Upload a file&rsquo;.</li>
<li>Locate the example data file you downloaded and select it.</li>
<li>A progress meter will appear. The upload may take a few moments.</li>
</ul>

<p>When it is complete you&rsquo;ll see this:</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/uploaded_stock_data.jpg?raw=true" alt="Uploaded Stock Data" />
</p>

<p>Now click the file name &ldquo;NYSE-2000-2001.tar.gz&rdquo;. You&rsquo;ll see it, displayed in tabular form:</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/view_stock_data.jpg?raw=true" alt="View Stock Data" />
</p>

<p>You can use File Browser just like your own computer&rsquo;s file manager. Next register the dataset with HCatalog.</p>

<h3 id="loading-the-sample-data-into-hcatalog:c4646bf119986fba0efcf7fe4daeae11">Loading the sample data into HCatalog</h3>

<p>Now that we&rsquo;ve uploaded a file to HDFS, we will register it with HCatalog to be able to access it in both Pig and Hive.</p>

<p>Select the HCatalog icon in the icon bar at the top of the page:</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/select_hcat.jpg?raw=true" alt="Select HCat" />
</p>

<p>Select &ldquo;Create a new table from file&rdquo; from the Actions menu on the left.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/hcat_create_table.jpg?raw=true" alt="HCat Create Table" />
</p>

<p>Fill in the Table Name field with &lsquo;nyse_stocks&rsquo;. Then click on Choose a file button. Select the file we just uploaded &lsquo;NYSE-2000-2001.tsv.gz&rsquo;.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/hcat_choose_file-b.jpg?raw=true" alt="HCat Choose File" />
</p>

<p>You will now see the options for importing your file into a table. The File options should be fine. In Table preview set all text type fields to Column Type &lsquo;string&rsquo; and all decimal fields (ex: 12.55) to Column Type &lsquo;float.&rsquo; The one exception is &lsquo;stock_volume&rsquo; field should be set as &lsquo;bigint.&rsquo; When everything is complete click on the &ldquo;Create Table&rdquo; button at the bottom.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/hcat_define_columns.jpg?raw=true" alt="HCat Define Columns" />
</p>

<h3 id="a-short-apache-hive-tutorial:c4646bf119986fba0efcf7fe4daeae11">A Short Apache Hive Tutorial</h3>

<p>In the previous sections you:</p>

<ul>
<li>Uploaded your data file into HDFS</li>
<li>Used Apache HCatalog to create a table</li>
</ul>

<p>Apache Hive™ provides a data warehouse function to the Hadoop cluster. Through the use of HiveQL you can view your data as a table and create queries like you would in a database.</p>

<p>To make it easy to interact with Hive we use a tool in the Hortonworks Sandbox called Beeswax. Beeswax gives us an interactive interface to Hive. We can type in queries and have Hive evaluate them for us using a series of MapReduce jobs.</p>

<p>Let&rsquo;s open Beeswax. Click on the bee icon on the top bar.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/1-beeswax.jpg?raw=true" alt="Beeswax" />
</p>

<p>On the right hand side there is a query window and an execute button. We will be typing our queries in the query window. When you are done with a query please click on the execute button. Note: There is a limitation of one query in the composition window. You can not type multiple queries separated by semicolons.</p>

<p>Since we created our table in HCatalog, Hive automatically knows about it. We can see the tables that Hive knows about by clicking on the Tables tab.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/2-tables.jpg?raw=true" alt="Tables" />
</p>

<p>In the list of the tables you will see our table, <code>nyse_stocks</code>. Hive inherits the schema and location information from HCatalog. This separates meta information like schema and location from the queries. If we did not have HCatalog we would have to build the table by providing location and schema information.</p>

<p>We can see the records by typing <code>Select * from nyse_stocks</code> in the Query window. Our results would be:</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/3-data-table.jpg?raw=true" alt="Data Table" />
</p>

<p>We can see the columns in the table by executing <code>describe nyse_stocks</code></p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/4-describe-nyse_stocks.jpg?raw=true" alt="NYSE" />
</p>

<p>We will then get a description of the nyse table.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/5-describe-nyse_stocks-results.jpg?raw=true" alt="Describe NYSE Table" />
</p>

<p>We can count the records with the query <code>select count(*) from nyse_stocks</code>. You can click on the Beeswax icon to get back to the query screen. Evaluate the expression by typing it in the query window and hitting execute.
I4HP4F</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/select-count.jpg?raw=true" alt="Select Count" />
</p>

<p>This job takes longer and you can watch the job running in the log. When the job is complete you will see the results posted in the Results tab.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/select-count-results.jpg?raw=true" alt="Select Count Results" />
</p>

<p>You can select specific records by using a query like <code>select * from nyse_stocks where stock_symbol=&quot;IBM&quot;</code>.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/select-IBM.jpg?raw=true" alt="Select IBM" />
</p>

<p>This will return the records with IBM.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/select-ibm-results.jpg?raw=true" alt="Select IBM Results" />
</p>

<p>So we have seen how we can use Apache Hive to easily query our data in HDFS using the Apache Hive query language. We took full advantage of HCatalog so we did not have to specify our schema or location of the data. Apache Hive allows people who are knowledgable in query languages like SQL to immediately become productive with Apache Hadoop. Once they know the schema of the data can they quickly and easily formulate queries.</p>

<h3 id="pig-basics-tutorial:c4646bf119986fba0efcf7fe4daeae11">Pig Basics Tutorial</h3>

<p>In this tutorial we create and run Pig scripts. On the left is a list of scripts that we have created. In the middle is an area for us to compose our scripts. We will also load the data from the table we have stored in HCatalog. We will then filter out the records for the stock symbol IBM. Once we have done that we will calculate the average of closing stock prices over this period.</p>

<p>The basic steps will be:</p>

<ul>
<li>Step 1: Create and name the script</li>
<li>Step 2: Loading the data</li>
<li>Step 3: Select all records starting with IBM</li>
<li>Step 4: iterate and average</li>
<li>Step 5: save the script and execute it</li>
</ul>

<p>Let&rsquo;s get started…</p>

<p>To get to the Pig interface click on the Pig icon on the icon bar at the top. This will bring up the Pig user interface. On the left is a list of your scripts and on the right is a composition box for your scripts.</p>

<p>A special feature of the interface is the Pig helper at the bottom. The Pig helper will provide us with templates for the statements, functions, I/O statements, HCatLoader() and Python user defined functions.</p>

<p>At the very bottom are status areas that will show the results of our script and log files</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/PigUI.jpeg" alt="PIG UI" />
</p>

<h3 id="step-1-create-and-name-the-script:c4646bf119986fba0efcf7fe4daeae11">Step 1: Create and name the script</h3>

<ul>
<li>Open the Pig interface by clicking the Pig icon at the top of the screen</li>
</ul>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/image001.1.jpg?raw=true" alt="Image001-1" />
</p>

<ul>
<li>Title your script by filling in the title box</li>
</ul>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/image001.2.jpg?raw=true" alt="Image001-2" />
</p>

<h3 id="step-2-loading-the-data:c4646bf119986fba0efcf7fe4daeae11">Step 2: Loading the data</h3>

<p>Our first line in the script will load the table. We are going to use HCatalog because this allows us to share schema across tools and users within our Hadoop environment. HCatalog allows us to factor out schema and location information from our queries and scripts and centralize them in a common repository. Since it is in HCatalog we can use the HCatLoader() function. Pig makes it easy by allowing us to give the table a name or alias and not have to worry about allocating space and defining the structure. We just have to worry about how we are processing the table.</p>

<ul>
<li>On the right hand side we can start adding our code at Line 1</li>
<li>We can use the Pig helper at the bottom of the screen to give us a template for the line. Click on <code>Pig helper -&gt; HCatalog-&gt;load template</code></li>
<li>The entry <code>%TABLE%</code> is highlighted in red for us. Type the name of the table which is<code>nyse_stocks</code>.</li>
<li>Remember to add the <code>a =</code> before the template. This saves the results into <code>a</code>. Note the `= has to have a space before and after it.</li>
</ul>

<p>Our completed line of code will look like:</p>

<p><code>a = LOAD ‘default.nyse_stocks’ USING org.apache.hive.hcatalog.pig.HCatLoader()</code></p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/line1.jpeg" alt="Line 1" />
</p>

<p>So now we have our table loaded into Pig and we stored it &ldquo;<code>a</code>&ldquo;</p>

<h3 id="step-3-select-all-records-starting-with-ibm:c4646bf119986fba0efcf7fe4daeae11">Step 3: Select all records starting with IBM</h3>

<p>The next step is to select a subset of the records so that we just have the records for stock ticker of IBM. To do this in Pig we use the Filter operator. We tell Pig to Filter our table and keep all records where stock_symbol=&ldquo;IBM&rdquo; and store this in b. With this one simple statement Pig will look at each record in the table and filter out all the ones that do not meet our criteria. The group statement is important because it groups the records by one or more relations. In this case we just specified all rather than specify the exact relation we need.</p>

<ul>
<li>We can use Pig Help again by clicking on <code>Pig helper-&gt;Relational Operators-&gt;FILTER</code>template</li>
<li>We can replace <code>%VAR%</code> with &ldquo;<code>a</code>&rdquo; (hint: tab jumps you to the next field)</li>
<li>Our <code>%COND% is &quot;</code>stock_symbol ==&lsquo;IBM&rsquo;` &ldquo; (note: single quotes are needed around IBM and don&rsquo;t forget the trailing semi-colon)</li>
<li><code>Pig helper -&gt; Relational Operators-&gt;GROUP ALL</code> template</li>
<li>The first <code>%VAR% is &quot;</code>b<code>&quot; and the second</code>%VAR%<code>is &quot;</code>all<code>&quot;. You will need to correct an irregularity in the Pig syntax here. Remove the &quot;</code>BY`&rdquo; in the line of code.</li>
<li>Again add the trailing semi-colon to the code.</li>
</ul>

<p>So the final code will look like:</p>

<p><code>b = filter a by stock_symbol == 'IBM';  
c = group b all;  
</code></p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/line3.jpeg" alt="Line 3" />
</p>

<p>Now we have extracted all the records with IBM as the stock_symbol.</p>

<h3 id="step-4-iterate-and-average:c4646bf119986fba0efcf7fe4daeae11">Step 4: Iterate and Average</h3>

<p>Now that we have the right set of records we can iterate through them and create the average. We use the &ldquo;foreach&rdquo; operator on the grouped data to iterate through all the records. The AVG() function creates the average of the stock_volume field. To wind it up we just print out the results which will be a single floating point number. If our results would be used for a future job we can save it back into a table.</p>

<ul>
<li><code>Pig helper -&gt;Relational Operators-&gt;FOREACH</code> template will get us the code</li>
<li>Our first <code>%VAR%</code> is <code>c</code> and the second <code>%VAR%</code> is &ldquo;<code>AVG(b.stock_volume);</code>&ldquo;</li>
<li>We add the last line with <code>Pig helper-&gt;I/O-&gt;DUMP</code> template and replace <code>%VAR%</code> with &ldquo;<code>d</code>&rdquo;.</li>
</ul>

<p>Our last two lines of the script will look like:</p>

<p><code>d = foreach c generate AVG(b.stock_volume);  
dump d;  
</code></p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/line5.jpeg" alt="Line 5" />
</p>

<p>So the variable &ldquo;<code>d</code>&rdquo; will contain the average volume of IBM stock when this<br />
line is executed.</p>

<h3 id="step-5-save-the-script-and-execute-it:c4646bf119986fba0efcf7fe4daeae11">Step 5: Save the script and Execute it</h3>

<p>In the pig arguments text box enter <code>-useHCatalog</code>.</p>

<p>Now, we can save our completed script using the Save button at the bottom and then we can Execute it. This will create a MapReduce job(s) and after it runs we will get our results. At the bottom there will be a progress bar that shows the job status.</p>

<ul>
<li>At the bottom we click on the Save button again</li>
<li>Then we click on the Execute button to run the script</li>
<li>Below the Execute button is a progress bar that will show you how things are running.</li>
<li>When the job completes you will see the results in the green box.</li>
<li>Click on the Logs link to see what happened when your script ran. This is where you will see any error messages. The log may scroll below the edge of your window so you may have to scroll down.</li>
</ul>

<p><img src="http://s3.amazonaws.com/hortonassets/Screen%20Shot%202015-01-29%20at%2010.28.35%20AM.png" alt="Final" />
</p>

<h3 id="summary:c4646bf119986fba0efcf7fe4daeae11">Summary</h3>

<p>Now we have a complete script that computes the average volume of IBM stock. You can download the results by clicking on the green download icon above the green box.</p>

<p><img src="http://raw.github.com/hortonworks/hadoop-tutorials/master/Sandbox/images/tutorial-1/answer.jpeg" alt="Answer" />
</p>

<p>If you look at what our script has done, you see in Line 5 we:</p>

<ul>
<li>Pulled in the data from our table using HCatalog, we took advantage that HCatalog provided us with location and schema information, if that needs to change in the future we would not have to rewrite our script.</li>
<li>Pig then went through all the rows in the table and discarded the ones where the stock_symbol field is not IBM</li>
<li>Then an index was built for the remaining records</li>
<li>The average of stock_volume was calculated on the records</li>
</ul>

<p>We did it with 5 lines of Pig script code!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/first/">
        first
      </a>
    </h1>

    <span class="post-date">Fri, Feb 6, 2015</span>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/posts/Simulating%20and%20transporting%20Realtime%20event%20stream%20with%20Apache%20Kafka/">
        
      </a>
    </h1>

    <span class="post-date">Mon, Jan 1, 0001</span>

    

<h1 id="new-simulating-and-transporting-realtime-event-stream-with-apache-kafka:630e43e8c7d3cf4336a36284dae8b65f">NEW: Simulating and transporting Realtime event stream with Apache Kafka</h1>

<p>This tutorial will show how geo-location information from trucks can be combined with sensors on roads which report real-time events like speeding, lane-departure, unsafe tailgating, and unsafe following distances. <a href="http://kafka.apache.org/"><strong>Apache Kafka</strong></a> can be used on the Hortonworks Data Platform to capture these data real-time events. In coming tutorials, we will persist them to different data stores/queue (HBase, HDFS, ActiveMQ) for further analysis.</p>

<p><a href="http://kafka.apache.org/"><strong>Apache Kafka</strong></a> is an open source messaging system designed for:</p>

<ul>
<li>Persistent messaging</li>
<li>High throughput</li>
<li>Distributed</li>
<li>Multi-client support</li>
<li>Real time
<img src="http://hortonassets.s3.amazonaws.com/mda/Screen+Shot+2014-07-08+at+9.24.54+PM.png" alt="Kafka Producer-Broker-Consumer" />
<br />
Kafka Producer-Broker-Consumer</li>
</ul>

<p>In this tutorial, you will learn the following topics:</p>

<ol>
<li>Install and Start Kafka on <a href="http://hortonworks.com/products/hortonworks-sandbox/">Hortonworks Sandbox</a>.</li>
<li>Review Kafka and ZooKeeper Configs</li>
<li>Create Kafka topics for Truck events.</li>
<li>Writing Kafka Producers for Truck events.</li>
</ol>

<h2 id="prerequisites:630e43e8c7d3cf4336a36284dae8b65f">Prerequisites</h2>

<p>A working Hadoop cluster : the easiest way to get a pre-configured and fully functional Hadoop cluster is to download the <a href="http://hortonworks.com/products/hortonworks-sandbox/">Hortonworks SandBox</a>.</p>

<h2 id="tutorial:630e43e8c7d3cf4336a36284dae8b65f">Tutorial</h2>

<h3 id="step-1:630e43e8c7d3cf4336a36284dae8b65f">Step 1:</h3>

<p><strong>Login to Hortonworks Sandbox.</strong></p>

<p>After downloading the Sandbox and running the VM, login to Ambari using the URL <a href="http://127.0.0.1:8080/">http://127.0.0.1:8080/</a>.</p>

<p>The username and password is <code>admin</code> and <code>admin</code>.</p>

<h3 id="step-2:630e43e8c7d3cf4336a36284dae8b65f">Step 2:</h3>

<p><strong>Setup Kafka.</strong></p>

<ol>
<li>After login to Ambari, select Actions -&gt; Add Service:<br /></li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image01.png" alt="" />
</p>

<ol>
<li>Select Kafka from the list of Services and click Next:<br /></li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image04.png" alt="" />
</p>

<ol>
<li>Keep clicking <code>Next</code> with the selcted defaults until you reach the following screen:<br /></li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image07.png" alt="" />
</p>

<p>Add logs.dir = <code>/tmp/kafka-logs</code></p>

<ol>
<li>Deploy<br /></li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image10.png" alt="" />
</p>

<ol>
<li>Check Progress<br /></li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image13.png" alt="" />
</p>

<p>After successful Start you may be asked to restart some dependent Services like Nagios. Please select the appropriate Services and click Restart.</p>

<p>4.Setting up Kafka on Sandbox with ZooKeeper.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/Screen+Shot+2014-07-08+at+10.33.50+PM.png" alt="Single Broker based Kakfa cluster" />
</p>

<p>Kafka provides the default and simple ZooKeeper configuration file used for launching a single local ZooKeeper instance. Here, ZooKeeper serves as the coordination interface between the Kafka broker and consumers.</p>

<p>The important Zookeeper properties can be checked in Ambari as below:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image16.png" alt="" />
</p>

<p>Ensure the following value of clientPort under Ambari</p>

<p>clientPort=2181. By default ZooKeeper server listens on port number 2181.</p>

<p>Also verify if zookeeper service is running:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image19.png" alt="" />
</p>

<p>If this port 2181 is busy or is consumed by other processes, then you could change the default port number of ZooKeeper to any other valid port number.</p>

<p>In case zookeeper is not running, you can start the Zookeeper service from Ambari:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image22.png" alt="" />
</p>

<h3 id="step-3:630e43e8c7d3cf4336a36284dae8b65f">Step 3:</h3>

<p><strong>Verify Kafka Broker.</strong></p>

<p>Verify the ‘zookeeper.connect’ parameter to point to port number 2181.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/kafka/image25.png" alt="" />
</p>

<p>we will SSH in to follow the rest of the steps.</p>

<p>ssh root@127.0.0.1 -p 2222;</p>

<p>the password is hadoop</p>

<p>Check running java processes using the “jps” command.</p>

<p>This command should list Kafka and QuorumPeerMain</p>

<p>jps</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/Check+Kafka+Running.png" alt="check Kafka running" />
<br />
check Kafka running</p>

<h3 id="step-4:630e43e8c7d3cf4336a36284dae8b65f">Step 4:</h3>

<p><code>cd /usr/hdp/2.2.0.0-1084/kafka</code></p>

<p><strong>Create Kafka topics for truck event.</strong></p>

<p>Execute this command to create Topics for ‘truckevent&rsquo;:</p>

<pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic truckevent
</code></pre>

<p>Note: Sometimes Kafka does not listen to localhost, you may need to use IP instead.</p>

<p>Check if topic ‘truckevent’ was created successfully with the following command:</p>

<pre><code>bin/kafka-topics.sh --list --zookeeper localhost:2181
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/Kafka+Topics.png" alt="Kafka Topics" />
<br />
Kafka Topics</p>

<h3 id="step-5:630e43e8c7d3cf4336a36284dae8b65f">Step 5:</h3>

<p><strong>Writing Kafka Producers for truck events.</strong></p>

<p>Producers are applications that create Messages and publish them to the Kafka broker for further consumption.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/Screen+Shot+2014-07-09+at+12.31.03+AM.png" alt="Kafka Producers for truck events" />
<br />
Kafka Producers for truck events</p>

<p>In this tutorial we shall use a Java API to produce Truck events.</p>

<p>The Java code in <code>TruckEventsProducer.java</code> will generate data with following columns:</p>

<pre><code> `driver_name` string,  
 `driver_id` string,  
 `route_name` string,  
 `route_id` string,  
 `truck_id` string,  
 `timestamp` string,  
 `longitude` string,  
 `latitude` string,  
 `violation` string,  
 `total_violations` string
</code></pre>

<p>This Java Truck events producer code uses <a href="http://www.nyc.gov/html/dot/downloads/misc/all_truck_routes_nyc.kml">New York City Truck Routes (kml)</a> file which defines road paths with Latitude and Longitude information.</p>

<p>The Truck Events Producer java code and the NYC Truck routes kml file can be downloaded with following commands.</p>

<pre><code>mkdir /opt/TruckEvents  
cd /opt/TruckEvents  
wget http://hortonassets.s3.amazonaws.com/mda/Tutorials-master.zip  
unzip Tutorials-master.zip
</code></pre>

<h3 id="step-6:630e43e8c7d3cf4336a36284dae8b65f">Step 6:</h3>

<p><strong>Compiling with Maven.</strong></p>

<p>Download and extract Apache Maven as shown in the commands below and set up the environment ‘PATH’ variable.</p>

<pre><code>wget http://www.carfab.com/apachesoftware/maven/maven-3/3.2.2/binaries/apache-maven-3.2.2-bin.tar.gz  
tar xvf apache-maven-3.2.2-bin.tar.gz  
mv apache-maven-3.2.2 /usr/local/  
export PATH=/usr/local/apache-maven-3.2.2/bin:$PATH  
mvn -version
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/maven+version.png" alt="Maven Version" />
<br />
Maven Version</p>

<p>Add <code>export PATH=/usr/local/apache-maven-3.2.2/bin:$PATH</code> to the <code>~/.bashrc</code> file to auto set the values for <code>$PATH</code>.</p>

<p>Now lets compile and execute the code to generate Truck Events.</p>

<pre><code>cd /opt/TruckEvents/Tutorials-master  
mvn clean package
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/mvn+clean+package.png" alt="mvn clean pacakge" />
<br />
mvn clean pacakge</p>

<p>Once the code is successfully compiled we shall see a new target directory created in the current folder. The binaries for all the Tutorials are in this target directory and the source code in src. To start the Kafka Producer we execute the following command to see the output as shown in the screenshot below.</p>

<pre><code>java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer localhost:9092 localhost:2181 &amp;
</code></pre>

<p><img src="http://hortonassets.s3.amazonaws.com/mda/start+kafka+producer.png" alt="TruckEventsProducer Running" />
<br />
TruckEventsProducer Running</p>

<p>We have now successfully compiled and have the Kafka producer publishing messages to the Kafka cluster.</p>

<p>To verify, execute the following command:</p>

<pre><code>[root@sandbox ]# /usr/hdp/2.2.0.0-1084/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic truckevent --from-beginning  
SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.  
SLF4J: Defaulting to no-operation (NOP) logger implementation  
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.  
2014-07-14 20:25:13.423|01|11|Unsafe following distance|    -74.01214432911161|40.70899264449202  
2014-07-14 20:25:14.193|02|12|Unsafe tail distance| -74.01214432911161|40.70899264449202  
2014-07-14 20:25:14.207|03|13|Overspeed|    -74.01214432911161|40.70899264449202  
2014-07-14 20:25:14.22|02|12|Overspeed| -74.01150567251823|40.70295884274092  
2014-07-14 20:25:14.236|03|13|Normal|   -74.01150567251823|40.70295884274092  
2014-07-14 20:25:14.253|02|12|Unsafe following distance|    -74.01150567251823|40.70295884274092  
2014-07-14 20:25:14.263|03|13|Lane Departure|   -74.01046533975128|40.71153454636318  
2014-07-14 20:25:14.271|02|12|Lane Departure|   -74.01046533975128|40.71153454636318
</code></pre>

<p>To stop kafka, you can use the command:</p>

<pre><code>service kafka stop
</code></pre>

<h2 id="producer-code-description:630e43e8c7d3cf4336a36284dae8b65f">Producer Code description</h2>

<p>We use the TruckEventsProducer.java file under the src/main/java/tutorial1/ directory to generate the Kafka TruckEvents. This uses the all_truck_routes_nyc.kml data file available from <a href="http://www.nyc.gov/html/dot/html/motorist/trucks.shtml">NYC DOT</a>. We use Java API’s to produce Truck Events.</p>

<pre><code>[root@sandbox ~]# ls /opt/TruckEvents/Tutorials-master/src/main/java/tutorial1/TruckEventsProducer.java  
[root@sandbox ~]# ls /opt/TruckEvents/Tutorials-master/src/main/resources/all_truck_routes_nyc.kml
</code></pre>

<p>The java file contains 3 functions</p>

<ul>
<li><strong>public class TruckEventsProducer</strong></li>
</ul>

<p>We configure the Kafka producer in this function to serialize and send the data to Kafka Topic ‘truckevent’ created in the tutorial. The code below shows the Producer <k, v="" style="box-sizing: border-box;">class used to generate messages.</p>

<pre><code>String TOPIC = &quot;truckevent&quot;;  
ProducerConfig config = new ProducerConfig(props);  
Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config);
</code></pre>

<p>The properties of the producer are defined in the ‘props’ variable. The events, truckIds and the driverIds data is selected with random function from the array variables.</p>

<pre><code>Properties props = new Properties();  
props.put(&quot;metadata.broker.list&quot;, args[0]);  
props.put(&quot;zk.connect&quot;, args[1]);  
props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);  
props.put(&quot;request.required.acks&quot;, &quot;1&quot;);



String[] events = {&quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Lane Departure&quot;, &quot;Overspeed&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Lane Departure&quot;,&quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;,  &quot;Unsafe tail distance&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Unsafe following distance&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Overspeed&quot;, &quot;Normal&quot;, &quot;Normal&quot;, };  
String[] truckIds = {&quot;01&quot;, &quot;02&quot;, &quot;03&quot;};  
String[] driverIds = {&quot;11&quot;, &quot;12&quot;, &quot;13&quot;};
</code></pre>

<p>KeyedMessage class takes the topic name, partition key, and the message value that needs to be passed from the producer as follows:</p>

<p><strong>class KeyedMessage<a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/val%20topic:%20String,%20val%20key:%20K,%20val%20message:%20V">K, V</a></strong></p>

<pre><code>KeyedMessage&lt;String, String&gt; data = new KeyedMessage&lt;String, String&gt;(TOPIC, finalEvent);
</code></pre>

<p>The Kafka producer events with timestamps are created by selecting the data from above arrays and geo location from the all_truck_routes_nyc.kml file.</p>

<pre><code>KeyedMessage&lt;String, String&gt; data = new KeyedMessage&lt;String, String&gt;(TOPIC, finalEvent);  
LOG.info(&quot;Sending Messge #:&quot; + i +&quot;, msg:&quot; + finalEvent);  
producer.send(data);  
Thread.sleep(1000);'
</code></pre>

<p>To transmit the data we now build an array using the GetKmlLangList() and getLatLong() function.</p>

<ul>
<li><strong>private static String getLatLong</strong></li>
</ul>

<p>This function returns coordinates in Latitude and Longitude format.</p>

<pre><code> if (latLong.length == -1)  
 {
    return latLong[1].trim() + &quot;|&quot; + latLong[0].trim();  
 }
</code></pre>

<ul>
<li><strong>public static String[] GetKmlLanLangList</strong></li>
</ul>

<p>This method is reading KML file which is an XML file. This xml file is loaded in File fXmlFile variable.</p>

<pre><code>File fXmlFile = new File(urlString);
</code></pre>

<p>Which will parse this file by running through each node (Node.ELEMENT_NODE) in loop. The XML element &ldquo;coordinates&rdquo; has array of two items lat, long. The function reads the lat, long and returns the values in array.</p>

<p>This tutorial gives you brief glimpse of how to use Apache Kafka to transport real-time events data.</p>

  </div>
  
</div>
</div>

  </body>
</html>
